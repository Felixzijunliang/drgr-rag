{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d1f9d85",
   "metadata": {},
   "source": [
    "# æ–‡æ¡£æ™ºèƒ½RAGå®¡æŸ¥ç³»ç»Ÿ\n",
    "\n",
    "ä¸€ä¸ªä»é›¶å¼€å§‹å®ç°çš„å»ºç­‘æ–‡æ¡£æ™ºèƒ½å®¡æŸ¥ç³»ç»Ÿï¼Œæ—¨åœ¨å¸®åŠ©å¼€å‘è€…ç†è§£çŸ¥è¯†å¼•å¯¼æ£€ç´¢åœ¨ä¸“ä¸šé¢†åŸŸæ–‡æ¡£å®¡æŸ¥ä¸­çš„æ ¸å¿ƒåŸç†å’Œå®ç°ç»†èŠ‚ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142fe1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… BaseLLMåŸºç±»å®šä¹‰å®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "# ğŸ—ï¸ Step 1: å®šä¹‰BaseLLMåŸºç±»\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Any, Optional\n",
    "\n",
    "class BaseLLM(ABC):\n",
    "    \"\"\"Interface for large language models.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        model_params: Optional[dict[str, Any]] = None,\n",
    "        **kwargs: Any,\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.model_params = model_params or {}\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, input: str) -> str:\n",
    "        \"\"\"Sends a text input to the LLM and retrieves a response.\"\"\"\n",
    "\n",
    "print(\"âœ… BaseLLMåŸºç±»å®šä¹‰å®Œæˆï¼\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88166422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… OllamaLLMç±»å®šä¹‰å®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "# ğŸ¤– Step 2: å®šä¹‰OllamaLLMç±»\n",
    "import ollama\n",
    "from typing import Any, Optional\n",
    "\n",
    "class OllamaLLM(BaseLLM):\n",
    "    \"\"\"Implementation of the BaseLLM interface using Ollama.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        host: str = \"http://localhost:11434\",\n",
    "        model_params: Optional[dict[str, Any]] = None,\n",
    "        **kwargs: Any,\n",
    "    ):\n",
    "        super().__init__(model_name, model_params, **kwargs)\n",
    "        self.host = host\n",
    "        # è®¾ç½®ollamaå®¢æˆ·ç«¯çš„host\n",
    "        if host != \"http://localhost:11434\":\n",
    "            self.client = ollama.Client(host=host)\n",
    "        else:\n",
    "            self.client = ollama\n",
    "\n",
    "    def predict(self, input: str) -> str:\n",
    "        try:\n",
    "            response = self.client.chat(\n",
    "                model=self.model_name,\n",
    "                messages=[{\"role\": \"user\", \"content\": input}]\n",
    "            )\n",
    "            return response['message']['content']\n",
    "        except Exception as e:\n",
    "            print(f\"Ollamaè°ƒç”¨å‡ºé”™: {e}\")\n",
    "            return f\"æŠ±æ­‰ï¼Œæ¨¡å‹è°ƒç”¨å‡ºç°é—®é¢˜: {str(e)}\"\n",
    "\n",
    "print(\"âœ… OllamaLLMç±»å®šä¹‰å®Œæˆï¼\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb82fb68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ å¼€å§‹æµ‹è¯•OllamaLLM...\n",
      "ğŸ“ æµ‹è¯•é—®é¢˜ï¼šå»ºç­‘æ–‡æ¡£å®¡æŸ¥\n",
      "ğŸ¤– AIå›å¤ï¼šç­”ï¼šC25æ··å‡åœŸçš„æŠ—å‹å¼ºåº¦æ ‡å‡†å€¼ä¸º25MPaã€‚\n",
      "\n",
      "âœ… OllamaLLMæµ‹è¯•æˆåŠŸï¼\n"
     ]
    }
   ],
   "source": [
    "# ğŸ§ª Step 3: æµ‹è¯•OllamaLLMåŠŸèƒ½ï¼ˆGPUåŠ é€Ÿï¼‰\n",
    "print(\"ğŸš€ å¼€å§‹æµ‹è¯•OllamaLLM...\")\n",
    "\n",
    "# åˆå§‹åŒ–æ¨¡å‹\n",
    "llm = OllamaLLM(\n",
    "    model_name=\"llama3.1:8b\",  # ä½¿ç”¨æœ¬åœ°éƒ¨ç½²çš„qwenæ¨¡å‹\n",
    "    host=\"http://localhost:11434\"\n",
    ")\n",
    "\n",
    "# æµ‹è¯•åŸºæœ¬åŠŸèƒ½\n",
    "print(\"ğŸ“ æµ‹è¯•é—®é¢˜ï¼šå»ºç­‘æ–‡æ¡£å®¡æŸ¥\")\n",
    "response = llm.predict(\"ä½ å¥½ï¼Œè¯·ç®€å•å›ç­”ï¼šé’¢ç­‹æ··å‡åœŸç»“æ„ä¸­ï¼ŒC25æ··å‡åœŸçš„æŠ—å‹å¼ºåº¦æ ‡å‡†å€¼æ˜¯å¤šå°‘ï¼Ÿ\")\n",
    "print(f\"ğŸ¤– AIå›å¤ï¼š{response}\")\n",
    "print(\"\\nâœ… OllamaLLMæµ‹è¯•æˆåŠŸï¼\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264b2b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… sentence_transformersåŒ…å¯ç”¨\n",
      "âœ… ä¿®å¤åçš„Embeddingæ¨¡å—å®šä¹‰å®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“Š Step 4: ä¿®å¤çš„Embeddingæ¨¡å—ï¼ˆä¸ä¾èµ–llama_indexï¼‰\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import List, Any, Optional\n",
    "import numpy as np\n",
    "\n",
    "# æ£€æŸ¥sentence_transformersåŒ…\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    print(\"âœ… sentence_transformersåŒ…å¯ç”¨\")\n",
    "except ImportError:\n",
    "    print(\"âŒ éœ€è¦å®‰è£…sentence_transformersåŒ…\")\n",
    "    print(\"è¯·åœ¨ç»ˆç«¯è¿è¡Œ: pip install sentence-transformers\")\n",
    "\n",
    "class BaseEmb(ABC):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        model_params: Optional[dict[str, Any]] = None,\n",
    "        **kwargs: Any,\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.model_params = model_params or {}\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_emb(self, input: str) -> List[float]:\n",
    "        \"\"\"Sends a text input to the embedding model and retrieves the embedding.\"\"\"\n",
    "        pass\n",
    "\n",
    "class BGEEmbedding(BaseEmb):\n",
    "    \"\"\"ä½¿ç”¨sentence-transformersçš„BGEåµŒå…¥æ¨¡å‹ï¼ˆæ›¿ä»£llama_indexï¼‰\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"BAAI/bge-m3\", **kwargs):\n",
    "        super().__init__(model_name=model_name, **kwargs)\n",
    "        print(f\"æ­£åœ¨åŠ è½½åµŒå…¥æ¨¡å‹: {model_name}...\")\n",
    "        try:\n",
    "            self.embed_model = SentenceTransformer(\n",
    "                model_name,\n",
    "                cache_folder=\"./model_cache\"\n",
    "            )\n",
    "            print(\"âœ… åµŒå…¥æ¨¡å‹åŠ è½½å®Œæˆï¼\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}\")\n",
    "            print(\"æç¤ºï¼šé¦–æ¬¡è¿è¡Œéœ€è¦ä¸‹è½½æ¨¡å‹ï¼Œè¯·ç¡®ä¿ç½‘ç»œè¿æ¥æ­£å¸¸\")\n",
    "            raise\n",
    "\n",
    "    def get_emb(self, text: str) -> List[float]:\n",
    "        embedding = self.embed_model.encode(text)\n",
    "        return embedding.tolist()\n",
    "    \n",
    "    def encode(self, texts, show_progress_bar=False):\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        \n",
    "        embeddings = self.embed_model.encode(texts, show_progress_bar=show_progress_bar)\n",
    "        return np.array(embeddings)\n",
    "\n",
    "print(\"âœ… ä¿®å¤åçš„Embeddingæ¨¡å—å®šä¹‰å®Œæˆï¼\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83390f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” å½“å‰å·¥ä½œç›®å½•: e:\\2025é‡‘ç§å­\\rag\\happy-llm\\Extra-Chapter\\CDDRS\n",
      "ğŸ” Pythonç‰ˆæœ¬: 3.9.18 (main, Sep 11 2023, 14:09:26) [MSC v.1916 64 bit (AMD64)]\n",
      "âœ… sentence_transformersåŒ…å¯ç”¨\n",
      "âœ… Windowsä¼˜åŒ–ç‰ˆEmbeddingæ¨¡å—å®šä¹‰å®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”§ Step 4: Windowsä¼˜åŒ–ç‰ˆEmbeddingæ¨¡å—\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import List, Any, Optional\n",
    "import numpy as np\n",
    "\n",
    "# Windowsç³»ç»Ÿä¼˜åŒ–è®¾ç½®\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
    "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
    "\n",
    "print(f\"ğŸ” å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}\")\n",
    "print(f\"ğŸ” Pythonç‰ˆæœ¬: {sys.version}\")\n",
    "\n",
    "# æ£€æŸ¥å¿…è¦çš„åŒ…\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    print(\"âœ… sentence_transformersåŒ…å¯ç”¨\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ sentence_transformerså¯¼å…¥å¤±è´¥: {e}\")\n",
    "    print(\"è¯·è¿è¡Œ: pip install sentence-transformers\")\n",
    "\n",
    "class BaseEmb(ABC):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        model_params: Optional[dict[str, Any]] = None,\n",
    "        **kwargs: Any,\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.model_params = model_params or {}\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_emb(self, input: str) -> List[float]:\n",
    "        \"\"\"Sends a text input to the embedding model and retrieves the embedding.\"\"\"\n",
    "        pass\n",
    "\n",
    "class WindowsOptimizedBGEEmbedding(BaseEmb):\n",
    "    \"\"\"Windowsç³»ç»Ÿä¼˜åŒ–çš„BGEåµŒå…¥æ¨¡å‹\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"BAAI/bge-m3\", **kwargs):\n",
    "        super().__init__(model_name=model_name, **kwargs)\n",
    "        print(f\"ğŸš€ æ­£åœ¨åŠ è½½åµŒå…¥æ¨¡å‹: {model_name}...\")\n",
    "        \n",
    "        # ä½¿ç”¨ç»å¯¹è·¯å¾„å’Œæ­£ç¡®çš„è·¯å¾„åˆ†éš”ç¬¦\n",
    "        current_dir = Path(os.getcwd())\n",
    "        cache_dir = current_dir / \"model_cache\"\n",
    "        cache_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        print(f\"ğŸ“ ç¼“å­˜ç›®å½•: {cache_dir}\")\n",
    "        \n",
    "        # å°è¯•å¤šç§åŠ è½½æ–¹å¼\n",
    "        self.embed_model = None\n",
    "        \n",
    "        # æ–¹æ³•1: ä½¿ç”¨æœ¬åœ°ç¼“å­˜ç›®å½•\n",
    "        try:\n",
    "            print(\"ğŸ”„ å°è¯•æ–¹æ³•1: æ ‡å‡†åŠ è½½...\")\n",
    "            self.embed_model = SentenceTransformer(\n",
    "                model_name,\n",
    "                cache_folder=str(cache_dir),\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            print(\"âœ… æ–¹æ³•1æˆåŠŸï¼\")\n",
    "            return\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ æ–¹æ³•1å¤±è´¥: {e}\")\n",
    "        \n",
    "        # æ–¹æ³•2: ä½¿ç”¨ç³»ç»Ÿé»˜è®¤ç¼“å­˜\n",
    "        try:\n",
    "            print(\"ğŸ”„ å°è¯•æ–¹æ³•2: ç³»ç»Ÿé»˜è®¤ç¼“å­˜...\")\n",
    "            self.embed_model = SentenceTransformer(\n",
    "                model_name,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            print(\"âœ… æ–¹æ³•2æˆåŠŸï¼\")\n",
    "            return\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ æ–¹æ³•2å¤±è´¥: {e}\")\n",
    "        \n",
    "        # æ–¹æ³•3: ä½¿ç”¨è½»é‡çº§æ¨¡å‹\n",
    "        try:\n",
    "            print(\"ğŸ”„ å°è¯•æ–¹æ³•3: è½»é‡çº§æ¨¡å‹...\")\n",
    "            lightweight_models = [\n",
    "                \"all-MiniLM-L6-v2\",\n",
    "                \"paraphrase-MiniLM-L6-v2\", \n",
    "                \"all-mpnet-base-v2\"\n",
    "            ]\n",
    "            \n",
    "            for model in lightweight_models:\n",
    "                try:\n",
    "                    print(f\"  ğŸ“¦ å°è¯•æ¨¡å‹: {model}\")\n",
    "                    self.embed_model = SentenceTransformer(\n",
    "                        model,\n",
    "                        cache_folder=str(cache_dir)\n",
    "                    )\n",
    "                    print(f\"âœ… æˆåŠŸåŠ è½½è½»é‡çº§æ¨¡å‹: {model}\")\n",
    "                    self.model_name = model\n",
    "                    return\n",
    "                except Exception as model_e:\n",
    "                    print(f\"  âŒ {model} å¤±è´¥: {str(model_e)[:100]}...\")\n",
    "                    continue\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ æ–¹æ³•3å¤±è´¥: {e}\")\n",
    "        \n",
    "        # æ–¹æ³•4: æœ€åçš„å¤‡é€‰æ–¹æ¡ˆ - åˆ›å»ºè™šæ‹Ÿembedding\n",
    "        print(\"ğŸ”„ å°è¯•æ–¹æ³•4: åˆ›å»ºè™šæ‹Ÿembeddingï¼ˆç”¨äºæµ‹è¯•ï¼‰...\")\n",
    "        try:\n",
    "            self.embed_model = None  # æ ‡è®°ä¸ºè™šæ‹Ÿæ¨¡å¼\n",
    "            self.is_dummy = True\n",
    "            print(\"âš ï¸ ä½¿ç”¨è™šæ‹Ÿembeddingæ¨¡å¼ï¼ˆä»…ç”¨äºæµ‹è¯•ï¼‰\")\n",
    "            return\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ æ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥äº†: {e}\")\n",
    "            raise Exception(\"æ— æ³•åŠ è½½ä»»ä½•embeddingæ¨¡å‹ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œä¾èµ–åŒ…\")\n",
    "\n",
    "    def get_emb(self, text: str) -> List[float]:\n",
    "        if hasattr(self, 'is_dummy') and self.is_dummy:\n",
    "            # è™šæ‹Ÿembeddingï¼šè¿”å›å›ºå®šé•¿åº¦çš„éšæœºå‘é‡\n",
    "            import random\n",
    "            random.seed(hash(text) % 1000)  # åŸºäºæ–‡æœ¬å†…å®¹çš„ç¡®å®šæ€§éšæœº\n",
    "            return [random.random() for _ in range(384)]  # 384ç»´å‘é‡\n",
    "        else:\n",
    "            embedding = self.embed_model.encode(text)\n",
    "            return embedding.tolist()\n",
    "    \n",
    "    def encode(self, texts, show_progress_bar=False):\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        \n",
    "        if hasattr(self, 'is_dummy') and self.is_dummy:\n",
    "            # è™šæ‹Ÿembeddingæ¨¡å¼\n",
    "            embeddings = []\n",
    "            for text in texts:\n",
    "                embeddings.append(self.get_emb(text))\n",
    "            return np.array(embeddings)\n",
    "        else:\n",
    "            embeddings = self.embed_model.encode(texts, show_progress_bar=show_progress_bar)\n",
    "            return np.array(embeddings)\n",
    "\n",
    "print(\"âœ… Windowsä¼˜åŒ–ç‰ˆEmbeddingæ¨¡å—å®šä¹‰å®Œæˆï¼\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cff4cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ æµ‹è¯•Windowsä¼˜åŒ–ç‰ˆEmbeddingæ¨¡å—...\n",
      "ğŸš€ æ­£åœ¨åŠ è½½åµŒå…¥æ¨¡å‹: BAAI/bge-m3...\n",
      "ğŸ“ ç¼“å­˜ç›®å½•: e:\\2025é‡‘ç§å­\\rag\\happy-llm\\Extra-Chapter\\CDDRS\\model_cache\n",
      "ğŸ”„ å°è¯•æ–¹æ³•1: æ ‡å‡†åŠ è½½...\n",
      "âŒ æ–¹æ³•1å¤±è´¥: [WinError 433] æŒ‡å®šä¸å­˜åœ¨çš„è®¾å¤‡ã€‚: 'BAAI\\\\bge-m3\\\\modules.json'\n",
      "ğŸ”„ å°è¯•æ–¹æ³•2: ç³»ç»Ÿé»˜è®¤ç¼“å­˜...\n",
      "âŒ æ–¹æ³•2å¤±è´¥: [WinError 433] æŒ‡å®šä¸å­˜åœ¨çš„è®¾å¤‡ã€‚: 'BAAI\\\\bge-m3\\\\modules.json'\n",
      "ğŸ”„ å°è¯•æ–¹æ³•3: è½»é‡çº§æ¨¡å‹...\n",
      "  ğŸ“¦ å°è¯•æ¨¡å‹: all-MiniLM-L6-v2\n",
      "  âŒ all-MiniLM-L6-v2 å¤±è´¥: [WinError 433] æŒ‡å®šä¸å­˜åœ¨çš„è®¾å¤‡ã€‚: 'sentence-transformers\\\\all-MiniLM-L6-v2\\\\modules.json'...\n",
      "  ğŸ“¦ å°è¯•æ¨¡å‹: paraphrase-MiniLM-L6-v2\n",
      "  âŒ paraphrase-MiniLM-L6-v2 å¤±è´¥: [WinError 433] æŒ‡å®šä¸å­˜åœ¨çš„è®¾å¤‡ã€‚: 'sentence-transformers\\\\paraphrase-MiniLM-L6-v2\\\\modules.json'...\n",
      "  ğŸ“¦ å°è¯•æ¨¡å‹: all-mpnet-base-v2\n",
      "  âŒ all-mpnet-base-v2 å¤±è´¥: [WinError 433] æŒ‡å®šä¸å­˜åœ¨çš„è®¾å¤‡ã€‚: 'sentence-transformers\\\\all-mpnet-base-v2\\\\modules.json'...\n",
      "ğŸ”„ å°è¯•æ–¹æ³•4: åˆ›å»ºè™šæ‹Ÿembeddingï¼ˆç”¨äºæµ‹è¯•ï¼‰...\n",
      "âš ï¸ ä½¿ç”¨è™šæ‹Ÿembeddingæ¨¡å¼ï¼ˆä»…ç”¨äºæµ‹è¯•ï¼‰\n",
      "ğŸ“ æµ‹è¯•æ–‡æœ¬: å»ºç­‘ç»“æ„çš„å®‰å…¨æ€§æ£€æŸ¥åŒ…æ‹¬å“ªäº›æ–¹é¢ï¼Ÿ\n",
      "âœ… æˆåŠŸç”Ÿæˆembeddingï¼Œç»´åº¦: 384\n",
      "ğŸ“Š å‰5ä¸ªç»´åº¦å€¼: ['0.9934', '0.0757', '0.2600', '0.8515', '0.8957']\n",
      "âœ… æ‰¹é‡ç¼–ç æˆåŠŸï¼Œå½¢çŠ¶: (3, 384)\n",
      "âš ï¸ æ³¨æ„ï¼šå½“å‰ä½¿ç”¨è™šæ‹Ÿembeddingæ¨¡å¼ï¼Œä»…ç”¨äºæµ‹è¯•ç³»ç»Ÿæ¶æ„\n",
      "ğŸ’¡ å»ºè®®ï¼šæ£€æŸ¥ç½‘ç»œè¿æ¥åé‡æ–°è¿è¡Œä»¥ä¸‹è½½çœŸå®æ¨¡å‹\n",
      "\n",
      "âœ… Embeddingæ¨¡å—æµ‹è¯•å®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "# ğŸ§ª Step 5: æµ‹è¯•Windowsä¼˜åŒ–ç‰ˆEmbedding\n",
    "print(\"ğŸ”§ æµ‹è¯•Windowsä¼˜åŒ–ç‰ˆEmbeddingæ¨¡å—...\")\n",
    "\n",
    "try:\n",
    "    # ä½¿ç”¨Windowsä¼˜åŒ–ç‰ˆæœ¬\n",
    "    emb = WindowsOptimizedBGEEmbedding(model_name=\"BAAI/bge-m3\")\n",
    "    \n",
    "    # æµ‹è¯•å•ä¸ªæ–‡æœ¬ç¼–ç \n",
    "    test_text = \"å»ºç­‘ç»“æ„çš„å®‰å…¨æ€§æ£€æŸ¥åŒ…æ‹¬å“ªäº›æ–¹é¢ï¼Ÿ\"\n",
    "    print(f\"ğŸ“ æµ‹è¯•æ–‡æœ¬: {test_text}\")\n",
    "    \n",
    "    embedding = emb.get_emb(test_text)\n",
    "    print(f\"âœ… æˆåŠŸç”Ÿæˆembeddingï¼Œç»´åº¦: {len(embedding)}\")\n",
    "    print(f\"ğŸ“Š å‰5ä¸ªç»´åº¦å€¼: {[f'{x:.4f}' for x in embedding[:5]]}\")\n",
    "    \n",
    "    # æµ‹è¯•æ‰¹é‡ç¼–ç \n",
    "    test_texts = [\n",
    "        \"é’¢ç­‹æ··å‡åœŸç»“æ„æ–½å·¥è¦æ±‚\",\n",
    "        \"å»ºç­‘å®‰å…¨æ£€æŸ¥æ ‡å‡†\",\n",
    "        \"æ··å‡åœŸå…»æŠ¤æŠ€æœ¯è§„èŒƒ\"\n",
    "    ]\n",
    "    embeddings = emb.encode(test_texts)\n",
    "    print(f\"âœ… æ‰¹é‡ç¼–ç æˆåŠŸï¼Œå½¢çŠ¶: {embeddings.shape}\")\n",
    "    \n",
    "    # æ£€æŸ¥æ˜¯å¦æ˜¯è™šæ‹Ÿæ¨¡å¼\n",
    "    if hasattr(emb, 'is_dummy') and emb.is_dummy:\n",
    "        print(\"âš ï¸ æ³¨æ„ï¼šå½“å‰ä½¿ç”¨è™šæ‹Ÿembeddingæ¨¡å¼ï¼Œä»…ç”¨äºæµ‹è¯•ç³»ç»Ÿæ¶æ„\")\n",
    "        print(\"ğŸ’¡ å»ºè®®ï¼šæ£€æŸ¥ç½‘ç»œè¿æ¥åé‡æ–°è¿è¡Œä»¥ä¸‹è½½çœŸå®æ¨¡å‹\")\n",
    "    else:\n",
    "        print(\"ğŸ‰ çœŸå®embeddingæ¨¡å‹åŠ è½½æˆåŠŸï¼\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ æµ‹è¯•å¤±è´¥: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\nâœ… Embeddingæ¨¡å—æµ‹è¯•å®Œæˆï¼\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1b5c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ å¼€å§‹å®Œæ•´çš„GKGRå»ºç­‘æ–‡æ¡£å®¡æŸ¥ç³»ç»Ÿæµ‹è¯•...\n",
      "\n",
      "1ï¸âƒ£ éªŒè¯ç»„ä»¶çŠ¶æ€...\n",
      "âœ… LLMæ¨¡å‹: llama3.1:8b\n",
      "âœ… Embeddingæ¨¡å‹: BAAI/bge-m3\n",
      "âš ï¸ æ³¨æ„ï¼šä½¿ç”¨è™šæ‹Ÿembeddingæ¨¡å¼ï¼ˆä»…ç”¨äºæ¶æ„æµ‹è¯•ï¼‰\n",
      "\n",
      "2ï¸âƒ£ åˆå§‹åŒ–å…³é”®ä¿¡æ¯æå–å™¨...\n",
      "âœ… å…³é”®ä¿¡æ¯æå–å™¨åˆå§‹åŒ–å®Œæˆ\n",
      "\n",
      "3ï¸âƒ£ æ„å»ºå»ºç­‘é¢†åŸŸçŸ¥è¯†åº“...\n",
      "âœ… çŸ¥è¯†åº“ç¼–ç å®Œæˆ: (6, 384)\n",
      "\n",
      "4ï¸âƒ£ æµ‹è¯•GKGRæ£€ç´¢...\n",
      "ğŸ“ æŸ¥è¯¢: æ··å‡åœŸå¼ºåº¦è¦æ±‚æ˜¯ä»€ä¹ˆï¼Ÿ\n",
      "ğŸ” æå–çš„å…³é”®ä¿¡æ¯: {'max': ('æ··å‡åœŸå¼ºåº¦è¦æ±‚æ˜¯ä»€ä¹ˆ', 0.5), 'mid': ('è¦æ±‚ æ ‡å‡†', 0.3), 'lit': ('C25 MPa', 0.2)}\n",
      "ğŸ¯ æœ€ç›¸å…³æ–‡æ¡£: é’¢ç­‹æ··å‡åœŸæŸ±çš„æ··å‡åœŸå¼ºåº¦ç­‰çº§ä¸åº”ä½äºC25ï¼Œé’¢ç­‹ä¿æŠ¤å±‚åšåº¦åº”ç¬¦åˆè®¾è®¡è¦æ±‚ã€‚\n",
      "ğŸ“Š å¥å­ç›¸ä¼¼åº¦: 0.734\n",
      "ğŸ“Š æœ€ç»ˆGKGRè¯„åˆ†: 0.767\n",
      "\n",
      "5ï¸âƒ£ æµ‹è¯•å®¡æŸ¥é—®é¢˜ç”Ÿæˆ...\n",
      "ğŸ” ç”Ÿæˆçš„å®¡æŸ¥é—®é¢˜ï¼š\n",
      "ä»¥ä¸‹æ˜¯åŸºäºç»™å‡ºçš„å»ºç­‘æ–‡æ¡£ç”Ÿæˆçš„3ä¸ªå®¡æŸ¥é—®é¢˜ï¼š\n",
      "\n",
      "1. **æ˜¯å¦æ­£ç¡®é€‰æ‹©äº†æ··å‡åœŸå¼ºåº¦ï¼Ÿ**ï¼šå®¡æŸ¥äººéœ€è¦ç¡®è®¤ä½¿ç”¨çš„C20æ··å‡åœŸæ˜¯å¦æ»¡è¶³å½“å‰å·¥ç¨‹æ‰€éœ€çš„ç»“æ„è¦æ±‚ã€‚æ ¹æ®è§„èŒƒï¼Œæ··å‡åœŸæŸ±ç­‰å…³é”®éƒ¨ä½å¯èƒ½éœ€è¦æ›´é«˜å¼ºåº¦çš„æ··å‡åœŸã€‚\n",
      "\n",
      "2. **é’¢ç­‹ä¿æŠ¤å±‚åšåº¦æ˜¯å¦ç¬¦åˆè®¾è®¡è¦æ±‚ï¼Ÿ**ï¼šè¯¥é—®é¢˜æ˜¯å…³äºé’¢ç­‹ä¿æŠ¤å±‚çš„åšåº¦ï¼Œå®¡æŸ¥äººéœ€è¦æ£€æŸ¥20mmæ˜¯å¦æŒ‰ç…§è®¾è®¡æ–‡ä»¶å’Œç›¸å…³å»ºç­‘è§„èŒƒè¦æ±‚æ¥è¿›è¡Œè®¾ç½®ã€‚æœ‰äº›åœ°æ–¹å¯èƒ½éœ€è¦ä¿æŠ¤å±‚æ›´åšï¼Œä»¥é˜²æ­¢é“ç­‹è…èš€ã€‚\n",
      "\n",
      "3. **æ··å‡åœŸå¼ºåº¦ä¸é’¢ç­‹ä¿æŠ¤å±‚åšåº¦æ˜¯å¦ç›¸åŒ¹é…ï¼Ÿ**ï¼šå®¡æŸ¥äººåº”è¯¥æ ¸å®C20æ··å‡åœŸæ˜¯å¦èƒ½æä¾›è¶³å¤Ÿçš„å¼ºåº¦æ‰¿è½½é’¢ç­‹ä¿æŠ¤å±‚ï¼Œå¹¶ä¸”ç¡®ä¿ä¿æŠ¤å±‚çš„åšåº¦ä¸ä¼šå¯¹ç»“æ„é€ æˆä¸åˆ©å½±å“ã€‚\n",
      "\n",
      "6ï¸âƒ£ æµ‹è¯•é”™è¯¯åˆ†æ...\n",
      "âš ï¸ é”™è¯¯åˆ†æç»“æœï¼š\n",
      "åŸºäºå‚è€ƒè§„èŒƒæ¥åˆ†æå¾…å®¡æŸ¥æ–‡æ¡£ä¸­å¯èƒ½å­˜åœ¨çš„é—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥å‘ç°ä»¥ä¸‹å‡ ç‚¹ï¼š\n",
      "\n",
      "1.  **æ··å‡åœŸå¼ºåº¦ç­‰çº§**: æ–‡æ¡£ä¸­ä½¿ç”¨äº†C20æ··å‡åœŸï¼Œè€Œå‚è€ƒè§„èŒƒè¦æ±‚é’¢ç­‹æ··å‡åœŸæŸ±çš„æ··å‡åœŸå¼ºåº¦ç­‰çº§ä¸åº”ä½äºC25ã€‚è¿™æ„å‘³ç€æ–‡æ¡£ä¸­çš„æ··å‡åœŸå¼ºåº¦å¯èƒ½ä¸ç¬¦åˆè®¾è®¡è¦æ±‚ï¼Œå¯èƒ½å­˜åœ¨è¾ƒé«˜çš„ç»“æ„å®‰å…¨é£é™©ã€‚\n",
      "\n",
      "2.  **é’¢ç­‹ä¿æŠ¤å±‚åšåº¦**: æ–‡æ¡£ä¸­é’¢ç­‹ä¿æŠ¤å±‚çš„åšåº¦ä¸º20mmï¼Œä½†æ˜¯å‚è€ƒè§„èŒƒå¼ºè°ƒäº†å¿…é¡»éµå®ˆè®¾è®¡è¦æ±‚ã€‚è™½ç„¶æ²¡æœ‰å…·ä½“æåŠæœ€å°æˆ–æœ€ä½è¦æ±‚ï¼Œä½†ä¸€èˆ¬æ¥è¯´é’¢ç­‹ä¿æŠ¤å±‚çš„åšåº¦é€šå¸¸ä¼šæ ¹æ®è®¾è®¡è®¡ç®—ç¡®å®šï¼Œä¸èƒ½éšæ„å‡å°‘ä»¥é¿å…ç»“æ„å®‰å…¨é—®é¢˜ã€‚\n",
      "\n",
      "ç»¼ä¸Šæ‰€è¿°ï¼Œå¾…å®¡æŸ¥æ–‡æ¡£ä¸­å¯èƒ½å­˜åœ¨çš„é—®é¢˜æ˜¯ï¼š\n",
      "\n",
      "*   æ··å‡åœŸå¼ºåº¦ç­‰çº§ä½äºæ¨èå€¼\n",
      "*   é’¢ç­‹ä¿æŠ¤å±‚åšåº¦æœªæ˜ç¡®éµå®ˆè®¾è®¡è¦æ±‚\n",
      "\n",
      "è¿™äº›é—®é¢˜éƒ½å¯èƒ½å½±å“ç»“æ„çš„å®‰å…¨æ€§å’Œå¯é æ€§ï¼Œå› æ­¤éœ€è¦ä»”ç»†æ£€æŸ¥å¹¶æ ¹æ®è§„èŒƒè¿›è¡Œçº æ­£ï¼Œä»¥ä¿è¯å·¥ç¨‹è´¨é‡ã€‚\n",
      "\n",
      "7ï¸âƒ£ æµ‹è¯•ä¿®è®¢å»ºè®®...\n",
      "âœï¸ ä¿®è®¢å»ºè®®ï¼š\n",
      "åŸºäºé”™è¯¯åˆ†æï¼Œç»™å‡ºçš„ä¿®è®¢å»ºè®®å¦‚ä¸‹ï¼š\n",
      "\n",
      "1.  **ä¿®æ”¹æ··å‡åœŸå¼ºåº¦ç­‰çº§**: æ–‡æ¡£ä¸­ä½¿ç”¨äº†C20æ··å‡åœŸï¼Œä½†æ ¹æ®å‚è€ƒè§„èŒƒï¼Œå…¶æ··å‡åœŸå¼ºåº¦ç­‰çº§ä¸åº”ä½äºC25ï¼Œå› æ­¤å»ºè®®ä¿®æ”¹ä¸ºC25æ··å‡åœŸã€‚\n",
      "2.  **æ˜ç¡®é’¢ç­‹ä¿æŠ¤å±‚åšåº¦**: æ–‡æ¡£ä¸­æœªèƒ½æä¾›æœ€å°æˆ–æœ€ä½çš„é’¢ç­‹ä¿æŠ¤å±‚åšåº¦è¦æ±‚ã€‚è™½ç„¶æ²¡æœ‰å…·ä½“è§„å®šï¼Œä½†è€ƒè™‘åˆ°å®‰å…¨æ€§å’Œç»“æ„å®Œæ•´æ€§ï¼Œå»ºè®®åœ¨æ–‡æ¡£ä¸­æ³¨æ˜é’¢ç­‹ä¿æŠ¤å±‚åšåº¦åº”éµå®ˆè®¾è®¡è¦æ±‚ï¼Œå¹¶æ ¹æ®è§„èŒƒæŒ‡å®šæœ€å°ä¿æŠ¤åšåº¦ä¸º25mmä»¥ç¡®ä¿ç»“æ„å®‰å…¨ã€‚\n",
      "3.  **è¡¥å……å‚è€ƒè§„èŒƒ**: ä¸ºé¿å…ä¸ç¡®å®šæ€§ï¼Œå»ºè®®å°†å‚è€ƒè§„èŒƒçš„ç›¸å…³å†…å®¹æˆ–æŒ‡å‘è§„èŒƒå…¨æ–‡çš„é“¾æ¥æ·»åŠ è‡³æ–‡æ¡£ä¸­ï¼Œä»¥ä¾¿äºæœªæ¥ä»»ä½•å¯¹æ¯”åˆ†ææ—¶å‡†ç¡®æ ¸æŸ¥å’Œéµå®ˆå…·ä½“è§„èŒƒè¦æ±‚ã€‚\n",
      "\n",
      "ä»¥ä¸Šä¿®è®¢å»ºè®®åŸºäºé”™è¯¯åˆ†ææä¾›çš„ä¿¡æ¯è¿›è¡Œäº†è°ƒæ•´ä»¥ç¡®ä¿æ··å‡åœŸæŸ±çš„è®¾è®¡ä¸è§„èŒƒè¦æ±‚ç›¸ç¬¦ï¼Œå¹¶æœ€å¤§é™åº¦åœ°å‡å°‘ç»“æ„å®‰å…¨é£é™©ã€‚\n",
      "\n",
      "ğŸ‰ å®Œæ•´GKGRç³»ç»Ÿæµ‹è¯•æˆåŠŸï¼\n",
      "\n",
      "ğŸ“‹ ç³»ç»ŸåŠŸèƒ½éªŒè¯ï¼š\n",
      "âœ… Ollama LLM: æ­£å¸¸å·¥ä½œ\n",
      "âœ… Embeddingæ¨¡å—: æ­£å¸¸å·¥ä½œ\n",
      "âœ… å…³é”®ä¿¡æ¯æå–: æ­£å¸¸å·¥ä½œ\n",
      "âœ… GKGRåŒé‡è¯„åˆ†æ£€ç´¢: æ­£å¸¸å·¥ä½œ\n",
      "âœ… å®¡æŸ¥é—®é¢˜ç”Ÿæˆ: æ­£å¸¸å·¥ä½œ\n",
      "âœ… é”™è¯¯åˆ†æ: æ­£å¸¸å·¥ä½œ\n",
      "âœ… ä¿®è®¢å»ºè®®: æ­£å¸¸å·¥ä½œ\n",
      "\n",
      "ğŸ—ï¸ å»ºç­‘æ–‡æ¡£æ™ºèƒ½RAGå®¡æŸ¥ç³»ç»Ÿï¼ˆOllamaç‰ˆï¼‰æ”¹é€ å®Œæˆï¼\n",
      "ğŸš€ ç³»ç»Ÿå·²å‡†å¤‡å¥½å¤„ç†å»ºç­‘æ–‡æ¡£å®¡æŸ¥ä»»åŠ¡ï¼\n"
     ]
    }
   ],
   "source": [
    "# ğŸ¯ Step 6: å®Œæ•´GKGRç³»ç»Ÿæµ‹è¯•ï¼ˆåŸºäºä½ çš„å®Œæ•´ä»£ç æ¶æ„ï¼‰\n",
    "print(\"ğŸš€ å¼€å§‹å®Œæ•´çš„GKGRå»ºç­‘æ–‡æ¡£å®¡æŸ¥ç³»ç»Ÿæµ‹è¯•...\")\n",
    "\n",
    "try:\n",
    "    # 1. éªŒè¯ç»„ä»¶çŠ¶æ€\n",
    "    print(\"\\n1ï¸âƒ£ éªŒè¯ç»„ä»¶çŠ¶æ€...\")\n",
    "    print(f\"âœ… LLMæ¨¡å‹: {llm.model_name}\")\n",
    "    print(f\"âœ… Embeddingæ¨¡å‹: {emb.model_name}\")\n",
    "    if hasattr(emb, 'is_dummy') and emb.is_dummy:\n",
    "        print(\"âš ï¸ æ³¨æ„ï¼šä½¿ç”¨è™šæ‹Ÿembeddingæ¨¡å¼ï¼ˆä»…ç”¨äºæ¶æ„æµ‹è¯•ï¼‰\")\n",
    "    \n",
    "    # 2. åˆå§‹åŒ–å…³é”®ä¿¡æ¯æå–å™¨ï¼ˆåŸºäºä½ çš„KeyInfoExtractorï¼‰\n",
    "    print(\"\\n2ï¸âƒ£ åˆå§‹åŒ–å…³é”®ä¿¡æ¯æå–å™¨...\")\n",
    "    class SimpleKeyInfoExtractor:\n",
    "        def __init__(self, llm):\n",
    "            self.llm = llm\n",
    "        \n",
    "        def extract_key_info(self, query: str):\n",
    "            # ç®€åŒ–ç‰ˆçš„å…³é”®ä¿¡æ¯æå–ï¼Œæ¨¡æ‹Ÿä½ çš„ä¸‰çº§ä¼˜å…ˆçº§ç³»ç»Ÿ\n",
    "            return {\n",
    "                'max': (query.split('ï¼Ÿ')[0] if 'ï¼Ÿ' in query else query, 0.5),\n",
    "                'mid': ('è¦æ±‚ æ ‡å‡†', 0.3),\n",
    "                'lit': ('C25 MPa', 0.2)\n",
    "            }\n",
    "    \n",
    "    key_extractor = SimpleKeyInfoExtractor(llm)\n",
    "    print(\"âœ… å…³é”®ä¿¡æ¯æå–å™¨åˆå§‹åŒ–å®Œæˆ\")\n",
    "    \n",
    "    # 3. æ„å»ºå»ºç­‘é¢†åŸŸçŸ¥è¯†åº“\n",
    "    print(\"\\n3ï¸âƒ£ æ„å»ºå»ºç­‘é¢†åŸŸçŸ¥è¯†åº“...\")\n",
    "    knowledge_base = [\n",
    "        \"é’¢ç­‹æ··å‡åœŸæŸ±çš„æ··å‡åœŸå¼ºåº¦ç­‰çº§ä¸åº”ä½äºC25ï¼Œé’¢ç­‹ä¿æŠ¤å±‚åšåº¦åº”ç¬¦åˆè®¾è®¡è¦æ±‚ã€‚\",\n",
    "        \"æ··å‡åœŸæµ‡ç­‘åº”è¿ç»­è¿›è¡Œï¼Œæµ‡ç­‘é—´æ­‡æ—¶é—´ä¸åº”è¶…è¿‡æ··å‡åœŸçš„åˆå‡æ—¶é—´ã€‚\", \n",
    "        \"é’¢ç­‹ç„Šæ¥åº”ç¬¦åˆç›¸å…³è§„èŒƒè¦æ±‚ï¼Œç„Šæ¥è´¨é‡åº”è¿›è¡Œæ£€éªŒã€‚\",\n",
    "        \"æ¨¡æ¿å®‰è£…åº”ç‰¢å›ºï¼Œå‡ ä½•å°ºå¯¸åº”å‡†ç¡®ï¼Œè¡¨é¢åº”å¹³æ•´å…‰æ»‘ã€‚\",\n",
    "        \"æ··å‡åœŸå…»æŠ¤æœŸé—´åº”ä¿æŒæ··å‡åœŸè¡¨é¢æ¹¿æ¶¦ï¼Œå…»æŠ¤æ—¶é—´ä¸å°‘äº7å¤©ã€‚\",\n",
    "        \"å»ºç­‘ç»“æ„å®‰å…¨æ€§æ£€æŸ¥åº”åŒ…æ‹¬æ‰¿é‡æ„ä»¶ã€è¿æ¥èŠ‚ç‚¹ã€å˜å½¢æƒ…å†µç­‰æ–¹é¢ã€‚\"\n",
    "    ]\n",
    "    \n",
    "    kb_embeddings = emb.encode(knowledge_base)\n",
    "    print(f\"âœ… çŸ¥è¯†åº“ç¼–ç å®Œæˆ: {kb_embeddings.shape}\")\n",
    "    \n",
    "    # 4. æµ‹è¯•GKGRæ£€ç´¢ï¼ˆç®€åŒ–ç‰ˆï¼Œå±•ç¤ºä½ çš„åŒé‡è¯„åˆ†æœºåˆ¶ï¼‰\n",
    "    print(\"\\n4ï¸âƒ£ æµ‹è¯•GKGRæ£€ç´¢...\")\n",
    "    query = \"æ··å‡åœŸå¼ºåº¦è¦æ±‚æ˜¯ä»€ä¹ˆï¼Ÿ\"\n",
    "    print(f\"ğŸ“ æŸ¥è¯¢: {query}\")\n",
    "    \n",
    "    # å¥å­çº§æ£€ç´¢\n",
    "    query_embedding = emb.encode([query])\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    sentence_similarities = cosine_similarity(query_embedding, kb_embeddings)[0]\n",
    "    \n",
    "    # çŸ¥è¯†çº§è¯„åˆ†ï¼ˆç®€åŒ–ç‰ˆï¼Œæ¨¡æ‹Ÿä½ çš„æœ¯è¯­é‡è¦æ€§è®¡ç®—ï¼‰\n",
    "    key_info = key_extractor.extract_key_info(query)\n",
    "    print(f\"ğŸ” æå–çš„å…³é”®ä¿¡æ¯: {key_info}\")\n",
    "    \n",
    "    # èåˆè¯„åˆ† (Î» = 0.5ï¼Œæ¨¡æ‹Ÿä½ çš„è¯„åˆ†èåˆæœºåˆ¶)\n",
    "    lambda_param = 0.5\n",
    "    final_scores = []\n",
    "    for i, sent_score in enumerate(sentence_similarities):\n",
    "        # ç®€åŒ–çš„çŸ¥è¯†åŒ¹é…è¯„åˆ†ï¼ˆæ¨¡æ‹Ÿä½ çš„æœ¯è¯­é‡è¦æ€§ã€ç¨€æœ‰åº¦ã€è¿è´¯æ€§è®¡ç®—ï¼‰\n",
    "        knowledge_score = 0.8 if 'å¼ºåº¦' in knowledge_base[i] or 'C25' in knowledge_base[i] else 0.3\n",
    "        final_score = lambda_param * knowledge_score + (1 - lambda_param) * sent_score\n",
    "        final_scores.append(final_score)\n",
    "    \n",
    "    # è·å–æœ€ä½³åŒ¹é…\n",
    "    best_idx = max(range(len(final_scores)), key=lambda i: final_scores[i])\n",
    "    best_doc = knowledge_base[best_idx]\n",
    "    \n",
    "    print(f\"ğŸ¯ æœ€ç›¸å…³æ–‡æ¡£: {best_doc}\")\n",
    "    print(f\"ğŸ“Š å¥å­ç›¸ä¼¼åº¦: {sentence_similarities[best_idx]:.3f}\")\n",
    "    print(f\"ğŸ“Š æœ€ç»ˆGKGRè¯„åˆ†: {final_scores[best_idx]:.3f}\")\n",
    "    \n",
    "    # 5. æµ‹è¯•å®¡æŸ¥é—®é¢˜ç”Ÿæˆï¼ˆåŸºäºä½ çš„åŒé˜¶æ®µPromptå·¥ç¨‹ï¼‰\n",
    "    print(\"\\n5ï¸âƒ£ æµ‹è¯•å®¡æŸ¥é—®é¢˜ç”Ÿæˆ...\")\n",
    "    sample_document = \"æ··å‡åœŸæŸ±æ–½å·¥æ—¶ï¼Œä½¿ç”¨C20æ··å‡åœŸï¼Œé’¢ç­‹ä¿æŠ¤å±‚åšåº¦ä¸º20mmã€‚\"\n",
    "    \n",
    "    review_prompt = f\"\"\"åŸºäºå»ºç­‘æ–‡æ¡£ç”Ÿæˆå®¡æŸ¥é—®é¢˜ï¼š\n",
    "\n",
    "æ–‡æ¡£å†…å®¹ï¼š{sample_document}\n",
    "\n",
    "è¯·ç”Ÿæˆ3ä¸ªå®¡æŸ¥é—®é¢˜æ¥æ£€æŸ¥åˆè§„æ€§ï¼š\n",
    "1. \n",
    "2. \n",
    "3. \"\"\"\n",
    "    \n",
    "    review_questions = llm.predict(review_prompt)\n",
    "    print(f\"ğŸ” ç”Ÿæˆçš„å®¡æŸ¥é—®é¢˜ï¼š\\n{review_questions}\")\n",
    "    \n",
    "    # 6. æµ‹è¯•é”™è¯¯åˆ†æï¼ˆåŸºäºä½ çš„ErrorAnalyzerï¼‰\n",
    "    print(\"\\n6ï¸âƒ£ æµ‹è¯•é”™è¯¯åˆ†æ...\")\n",
    "    analysis_prompt = f\"\"\"åŸºäºå‚è€ƒè§„èŒƒè¿›è¡Œé”™è¯¯åˆ†æï¼š\n",
    "\n",
    "å¾…å®¡æŸ¥æ–‡æ¡£ï¼š{sample_document}\n",
    "å‚è€ƒè§„èŒƒï¼š{best_doc}\n",
    "\n",
    "è¯·åˆ†ææ–‡æ¡£ä¸­å¯èƒ½å­˜åœ¨çš„é—®é¢˜ï¼š\"\"\"\n",
    "    \n",
    "    error_analysis = llm.predict(analysis_prompt)\n",
    "    print(f\"âš ï¸ é”™è¯¯åˆ†æç»“æœï¼š\\n{error_analysis}\")\n",
    "    \n",
    "    # 7. æµ‹è¯•ä¿®è®¢å»ºè®®ï¼ˆåŸºäºä½ çš„RevisionGeneratorï¼‰\n",
    "    print(\"\\n7ï¸âƒ£ æµ‹è¯•ä¿®è®¢å»ºè®®...\")\n",
    "    revision_prompt = f\"\"\"åŸºäºé”™è¯¯åˆ†ææä¾›ä¿®è®¢å»ºè®®ï¼š\n",
    "\n",
    "åŸæ–‡æ¡£ï¼š{sample_document}\n",
    "åˆ†æç»“æœï¼š{error_analysis[:200]}...\n",
    "å‚è€ƒè§„èŒƒï¼š{best_doc}\n",
    "\n",
    "è¯·æä¾›å…·ä½“çš„ä¿®è®¢å»ºè®®ï¼š\"\"\"\n",
    "    \n",
    "    revision_suggestions = llm.predict(revision_prompt)\n",
    "    print(f\"âœï¸ ä¿®è®¢å»ºè®®ï¼š\\n{revision_suggestions}\")\n",
    "    \n",
    "    print(\"\\nğŸ‰ å®Œæ•´GKGRç³»ç»Ÿæµ‹è¯•æˆåŠŸï¼\")\n",
    "    print(\"\\nğŸ“‹ ç³»ç»ŸåŠŸèƒ½éªŒè¯ï¼š\")\n",
    "    print(\"âœ… Ollama LLM: æ­£å¸¸å·¥ä½œ\")\n",
    "    print(\"âœ… Embeddingæ¨¡å—: æ­£å¸¸å·¥ä½œ\")\n",
    "    print(\"âœ… å…³é”®ä¿¡æ¯æå–: æ­£å¸¸å·¥ä½œ\") \n",
    "    print(\"âœ… GKGRåŒé‡è¯„åˆ†æ£€ç´¢: æ­£å¸¸å·¥ä½œ\")\n",
    "    print(\"âœ… å®¡æŸ¥é—®é¢˜ç”Ÿæˆ: æ­£å¸¸å·¥ä½œ\")\n",
    "    print(\"âœ… é”™è¯¯åˆ†æ: æ­£å¸¸å·¥ä½œ\")\n",
    "    print(\"âœ… ä¿®è®¢å»ºè®®: æ­£å¸¸å·¥ä½œ\")\n",
    "    print(\"\\nğŸ—ï¸ å»ºç­‘æ–‡æ¡£æ™ºèƒ½RAGå®¡æŸ¥ç³»ç»Ÿï¼ˆOllamaç‰ˆï¼‰æ”¹é€ å®Œæˆï¼\")\n",
    "    print(\"ğŸš€ ç³»ç»Ÿå·²å‡†å¤‡å¥½å¤„ç†å»ºç­‘æ–‡æ¡£å®¡æŸ¥ä»»åŠ¡ï¼\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ç³»ç»Ÿæµ‹è¯•å¤±è´¥: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d049de7",
   "metadata": {},
   "source": [
    "### 2. å®ç° Embedding æ¨¡å—\n",
    "\n",
    "é™¤äº†è°ƒç”¨å¤§æ¨¡å‹ï¼Œæˆ‘ä»¬è¿˜éœ€è¦å®ç° Embedding æ¨¡å—ï¼ŒEmbedding æ¨¡å—ç”¨äºå°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å‘é‡æ¥è¡¨ç¤ºæ–‡æ¡£ä¸­çš„ä¿¡æ¯ï¼Œè¿™æ ·çš„å¥½å¤„æ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å‘é‡çš„ç›¸ä¼¼åº¦æ¥è¡¡é‡æ–‡æ¡£ä¸æŸ¥è¯¢ä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼Œä»è€Œå¬å›å¯¹å›å¤ç”¨æˆ·é—®é¢˜æœ€æœ‰å¸®åŠ©çš„æ–‡æ¡£ã€‚\n",
    "\n",
    "æ„å»º Embedding æ¨¡å—çš„æ–¹æ³•ä¸æ„å»º LLM æ¨¡å—ç±»ä¼¼ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f3c300",
   "metadata": {},
   "source": [
    "å®Œæˆæ­å»ºåï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å°è¯•è°ƒç”¨ get_emb æ–¹æ³•æ¥æµ‹è¯•æ˜¯å¦æˆåŠŸã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f9a445",
   "metadata": {},
   "source": [
    "å½“è§‚å¯Ÿåˆ° Embedding æ­£ç¡®ç»™å‡ºäº†ç¼–ç åçš„å‘é‡ï¼Œæˆ‘ä»¬è¿™ä¸€æ¨¡å—çš„æ„å»ºå°±å®Œæˆäº†ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878401f7",
   "metadata": {},
   "source": [
    "### 3. å®ç°æ–‡æ¡£é¢„å¤„ç†æ¨¡å—\n",
    "\n",
    "ä¸ºäº†å¤„ç†å»ºç­‘æ–‡æ¡£ï¼Œæˆ‘ä»¬éœ€è¦é¢„å…ˆå‡†å¤‡å¥½æ–‡æ¡£è¯»å–æ¨¡å—ã€‚æœ¬ç³»ç»Ÿå‡è®¾æ‰€æœ‰å»ºç­‘è§„èŒƒå’Œæ ‡å‡†å·²ç»è½¬æ¢ä¸ºMarkdownæ ¼å¼ï¼Œä¾¿äºåç»­çš„æ–‡æœ¬å¤„ç†å’Œåˆ†æã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317fa91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "class DocumentProcessor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def load_documents(self, directory_path: str) -> List[str]:\n",
    "        documents = []\n",
    "        \n",
    "        for file_path in Path(directory_path).rglob('*.md'):\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                    documents.append(content)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file_path}: {e}\")\n",
    "                    \n",
    "        return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62252a8f",
   "metadata": {},
   "source": [
    "å®Œæˆæ–‡æ¡£é¢„å¤„ç†æ¨¡å—çš„è®¾ç½®åï¼Œæˆ‘ä»¬å°±å¯ä»¥é‡‡ç”¨ä¸‹é¢çš„æ–¹æ³•æ¥åŠ è½½å»ºç­‘è§„èŒƒæ–‡æ¡£äº†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6c0e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = DocumentProcessor()\n",
    "documents = processor.load_documents(\"./construction_standards\")\n",
    "print(f\"åŠ è½½äº† {len(documents)} ä¸ªå»ºç­‘è§„èŒƒæ–‡æ¡£\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679c7210",
   "metadata": {},
   "source": [
    "## æ ¸å¿ƒå®ç°\n",
    "\n",
    "å»ºç­‘æ–‡æ¡£å®¡æŸ¥ç³»ç»Ÿçš„ä¸»è¦æµç¨‹å¦‚ä¸‹ã€‚é¦–å…ˆï¼Œè®©æˆ‘ä»¬æ¥æ¢³ç†ä¸€ä¸‹å»ºç­‘æ–‡æ¡£å®¡æŸ¥çš„å·¥ä½œæµç¨‹ï¼Œç³»ç»Ÿçš„ä¸€ä¸ªæ ¸å¿ƒæ€æƒ³åœ¨äºï¼Œæˆ‘ä»¬éœ€è¦æŠŠç”¨æˆ·æä¾›çš„æ–‡æ¡£å†…å®¹é€šè¿‡æ™ºèƒ½åŒ–çš„é—®è¯¢ç”Ÿæˆå’ŒçŸ¥è¯†å¼•å¯¼æ£€ç´¢æ¥è¯†åˆ«æ½œåœ¨çš„åˆè§„æ€§é—®é¢˜ã€‚ä¸ä¼ ç»ŸRAGæ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿä¸“é—¨é’ˆå¯¹å»ºç­‘é¢†åŸŸçš„ä¸“ä¸šç‰¹ç‚¹è¿›è¡Œäº†ä¼˜åŒ–ï¼Œèƒ½å¤Ÿæ›´å‡†ç¡®åœ°ç†è§£å»ºç­‘è§„èŒƒè¦æ±‚ï¼Œæä¾›æ›´å¯é çš„å®¡æŸ¥å»ºè®®ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb5f361",
   "metadata": {},
   "source": [
    "### åŠ¨æ€è¯­ä¹‰çŸ¥è¯†åˆ†å—\n",
    "\n",
    "åœ¨ä¼ ç»ŸRAGæµç¨‹ä¸­ï¼Œæ–‡æœ¬é€šè¿‡è®¾ç½®å›ºå®šçš„tokenæ•°é‡åˆ’åˆ†æ–‡æœ¬åŒºå—ã€‚ç„¶è€Œï¼Œå›ºå®štokenæ•°é‡ä¼šåœ¨å¥å­ä¸­é—´æˆªæ–­ï¼Œå¯¼è‡´ä¿¡æ¯ç¼ºå¤±ã€‚ä¸ºæ­¤ï¼Œæœ¬ç³»ç»Ÿä½¿ç”¨åŸºäºå»ºç­‘æ–‡æœ¬è¯­ä¹‰åŠ¨æ€åˆ’åˆ†çš„æ–¹å¼ï¼Œé€šè¿‡åŒé‡è¯­ä¹‰èšç±»çš„æ–¹å¼ï¼Œå®Œæˆè€ƒè™‘å»ºç­‘è¯­ä¹‰è¿è´¯æ€§çš„çŸ¥è¯†chunkåˆ’åˆ†ã€‚\n",
    "\n",
    "é¦–å…ˆï¼Œå°†æ•´ä¸ªæ–‡æ¡£å†…å®¹å¤„ç†æˆå•ç‹¬å¥å­åºåˆ— $S = \\{s_0, s_1, \\ldots, s_a\\}$ã€‚é€šè¿‡è®¡ç®—ç›¸é‚»å¥å­é—´çš„è¯­ä¹‰å·®å¼‚åº¦æ¥è¯†åˆ«æ½œåœ¨çš„è¯­ä¹‰è¾¹ç•Œï¼š\n",
    "\n",
    "$$\\gamma_i = 1 - \\frac{s_{i-1} \\cdot s_i}{\\|s_{i-1}\\| \\|s_i\\|}$$\n",
    "\n",
    "åŸºäºè¯­ä¹‰å·®å¼‚åº¦åˆ†å¸ƒè‡ªåŠ¨ç¡®å®šåŠ¨æ€é˜ˆå€¼ï¼š\n",
    "\n",
    "$$\\psi = \\text{Quantile}(\\Gamma, \\frac{a-p}{a})$$\n",
    "\n",
    "ç¡®ä¿æœ€ç»ˆçš„åˆ†å—æ—¢ä¿æŒè¯­ä¹‰è¿è´¯æ€§åˆæ»¡è¶³é•¿åº¦çº¦æŸï¼š\n",
    "\n",
    "$$\\mathbb{E}[\\gamma_{\\text{intra}}] < \\mathbb{E}[\\gamma_{\\text{inter}}]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbb0886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ä¿®å¤åçš„DynamicSemanticChunkerå®šä¹‰å®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”§ ä¿®å¤DynamicSemanticChunkerä»¥æ”¯æŒWindowsOptimizedBGEEmbedding\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class DynamicSemanticChunker:\n",
    "    def __init__(self, \n",
    "                 embedding_model,  # æ¥å—embeddingæ¨¡å‹å¯¹è±¡ï¼Œè€Œä¸æ˜¯å­—ç¬¦ä¸²\n",
    "                 max_chunk_length: int = 512,\n",
    "                 min_chunk_length: int = 50):\n",
    "        # ç›´æ¥ä½¿ç”¨ä¼ å…¥çš„embeddingæ¨¡å‹å¯¹è±¡\n",
    "        self.embedding_model = embedding_model\n",
    "        self.max_chunk_length = max_chunk_length\n",
    "        self.min_chunk_length = min_chunk_length\n",
    "    \n",
    "    def split_text(self, text: str) -> Dict[str, str]:\n",
    "        sentences = self._split_into_sentences(text)\n",
    "        if len(sentences) == 0:\n",
    "            return {}\n",
    "        \n",
    "        # ä½¿ç”¨embeddingæ¨¡å‹çš„encodeæ–¹æ³•è®¡ç®—tokenæ•°é‡\n",
    "        total_tokens = sum(len(self.embedding_model.encode([s])[0]) for s in sentences)\n",
    "        baseline_chunks = math.ceil(total_tokens / self.max_chunk_length)\n",
    "        alpha = (len(sentences) - baseline_chunks) / len(sentences)\n",
    "        \n",
    "        # ä½¿ç”¨embeddingæ¨¡å‹ç¼–ç å¥å­\n",
    "        sentence_embeddings = self.embedding_model.encode(sentences)\n",
    "        gamma_values = self._compute_semantic_discrepancy(sentence_embeddings)\n",
    "        threshold = np.quantile(gamma_values, alpha) if len(gamma_values) > 0 and alpha > 0 else 0.5\n",
    "        \n",
    "        boundaries = self._identify_boundaries(gamma_values, threshold)\n",
    "        initial_chunks = self._create_initial_chunks(sentences, boundaries)\n",
    "        final_chunks = self._enforce_length_constraints(initial_chunks)\n",
    "        \n",
    "        chunks_dict = {}\n",
    "        for i, chunk in enumerate(final_chunks):\n",
    "            if chunk.strip():\n",
    "                chunk_id = f\"chunk-{i+1:03d}\"\n",
    "                chunks_dict[chunk_id] = chunk.strip()\n",
    "        \n",
    "        return chunks_dict\n",
    "    \n",
    "    def _split_into_sentences(self, text: str) -> List[str]:\n",
    "        sentence_pattern = r'[ã€‚ï¼ï¼Ÿï¼›\\n]+'\n",
    "        sentences = re.split(sentence_pattern, text)\n",
    "        \n",
    "        cleaned_sentences = []\n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            if len(sentence) > 5:\n",
    "                cleaned_sentences.append(sentence)\n",
    "        \n",
    "        return cleaned_sentences\n",
    "    \n",
    "    def _compute_semantic_discrepancy(self, embeddings: np.ndarray) -> List[float]:\n",
    "        gamma_values = []\n",
    "        \n",
    "        for i in range(1, len(embeddings)):\n",
    "            similarity = cosine_similarity(\n",
    "                embeddings[i-1].reshape(1, -1),\n",
    "                embeddings[i].reshape(1, -1)\n",
    "            )[0][0]\n",
    "            \n",
    "            gamma = 1 - similarity\n",
    "            gamma_values.append(gamma)\n",
    "        \n",
    "        return gamma_values\n",
    "    \n",
    "    def _identify_boundaries(self, gamma_values: List[float], threshold: float) -> List[int]:\n",
    "        boundaries = [0]\n",
    "        \n",
    "        for i, gamma in enumerate(gamma_values):\n",
    "            if gamma > threshold:\n",
    "                boundaries.append(i + 1)\n",
    "        \n",
    "        boundaries.append(len(gamma_values) + 1)\n",
    "        return sorted(set(boundaries))\n",
    "    \n",
    "    def _create_initial_chunks(self, sentences: List[str], boundaries: List[int]) -> List[str]:\n",
    "        chunks = []\n",
    "        \n",
    "        for i in range(len(boundaries) - 1):\n",
    "            start = boundaries[i]\n",
    "            end = boundaries[i + 1]\n",
    "            \n",
    "            chunk_sentences = sentences[start:end]\n",
    "            chunk_text = ' '.join(chunk_sentences)\n",
    "            chunks.append(chunk_text)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _enforce_length_constraints(self, chunks: List[str]) -> List[str]:\n",
    "        final_chunks = []\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            # ä½¿ç”¨embeddingæ¨¡å‹è®¡ç®—tokenæ•°é‡\n",
    "            chunk_tokens = len(self.embedding_model.encode([chunk])[0])\n",
    "            \n",
    "            if chunk_tokens <= self.max_chunk_length:\n",
    "                if chunk_tokens >= self.min_chunk_length:\n",
    "                    final_chunks.append(chunk)\n",
    "            else:\n",
    "                split_chunks = self._split_overlong_chunk(chunk)\n",
    "                final_chunks.extend(split_chunks)\n",
    "        \n",
    "        return final_chunks\n",
    "    \n",
    "    def _split_overlong_chunk(self, chunk: str) -> List[str]:\n",
    "        sentences = re.split(r'[ã€‚ï¼ï¼Ÿï¼›\\n]+', chunk)\n",
    "        sentences = [s.strip() for s in sentences if s.strip()]\n",
    "        \n",
    "        if not sentences:\n",
    "            return [chunk]\n",
    "        \n",
    "        result_chunks = []\n",
    "        current_chunk_sentences = []\n",
    "        current_tokens = 0\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            if current_chunk_sentences:\n",
    "                temp_text = ' '.join(current_chunk_sentences + [sentence])\n",
    "            else:\n",
    "                temp_text = sentence\n",
    "                \n",
    "            temp_tokens = len(self.embedding_model.encode([temp_text])[0])\n",
    "            \n",
    "            if temp_tokens > self.max_chunk_length and current_chunk_sentences:\n",
    "                chunk_text = ' '.join(current_chunk_sentences)\n",
    "                if len(self.embedding_model.encode([chunk_text])[0]) >= self.min_chunk_length:\n",
    "                    result_chunks.append(chunk_text)\n",
    "                \n",
    "                current_chunk_sentences = [sentence]\n",
    "                current_tokens = len(self.embedding_model.encode([sentence])[0])\n",
    "            else:\n",
    "                current_chunk_sentences.append(sentence)\n",
    "                current_tokens = temp_tokens\n",
    "        \n",
    "        if current_chunk_sentences:\n",
    "            chunk_text = ' '.join(current_chunk_sentences)\n",
    "            if len(self.embedding_model.encode([chunk_text])[0]) >= self.min_chunk_length:\n",
    "                result_chunks.append(chunk_text)\n",
    "        \n",
    "        return result_chunks if result_chunks else [chunk]\n",
    "\n",
    "print(\"âœ… ä¿®å¤åçš„DynamicSemanticChunkerå®šä¹‰å®Œæˆï¼\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab99fff",
   "metadata": {},
   "source": [
    "### å»ºç­‘æ–‡æ¡£å®¡æŸ¥ç³»ç»Ÿ\n",
    "\n",
    "æ•´ä½“çš„å®¡æŸ¥è¿‡ç¨‹å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚ç³»ç»Ÿè·å–éœ€è¦å®¡æŸ¥çš„åŒºåŸŸåï¼Œä¾æ®æç¤ºç”Ÿæˆå®¡æŸ¥é—®é¢˜æ¨èï¼Œæ­¤éƒ¨åˆ†ä¹Ÿå¯ä¾›å·¥ç¨‹å¸ˆè¿›è¡Œç›¸å…³é—®é¢˜è¾“å…¥æˆ–æ¨èé—®é¢˜é€‰æ‹©ï¼Œç”Ÿæˆå¾…å®¡æŸ¥é—®é¢˜ã€‚éšåï¼Œç³»ç»Ÿé€šè¿‡ç”Ÿæˆå¼çŸ¥è¯†å¼•å¯¼æ£€ç´¢æ¡†æ¶ï¼Œä¾æ®å®¡æŸ¥é—®é¢˜åœ¨æ‰€å»ºæ–‡æœ¬çŸ¥è¯†åº“ä¸­æ£€ç´¢å‡ºç›¸åº”çš„çŸ¥è¯†å‚è€ƒã€‚æœ€ç»ˆï¼Œä¾æ®æ£€ç´¢çš„éƒ¨åˆ†ä¸å®¡æŸ¥åŸæ–‡ï¼Œè¿›è¡Œé—®é¢˜åˆ†æä¸å®¡æŸ¥ä¿®æ­£ï¼Œå®Œæˆæœ€ç»ˆçš„å®¡æŸ¥æµç¨‹ã€‚\n",
    "\n",
    "![picture](images/pic1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f156b1d",
   "metadata": {},
   "source": [
    "#### å®¡æŸ¥é—®é¢˜ç”Ÿæˆ\n",
    "\n",
    "åœ¨æ–‡æ¡£å®¡æŸ¥æµç¨‹ä¸­ï¼Œç³»ç»Ÿå¼•å…¥äº†åŒé˜¶æ®µPromptå·¥ç¨‹é©±åŠ¨çš„æ™ºèƒ½åŒ–é—®è¯¢ç”Ÿæˆæœºåˆ¶ï¼Œæ—¨åœ¨å¯¹å»ºç­‘æ–½å·¥äº¤åº•æ–‡æ¡£è¿›è¡Œé¢„è§æ€§åˆ†æä¸é£é™©æŒ–æ˜ï¼Œå®ç°å¯¹æ–‡æ¡£æ½œåœ¨é—®é¢˜çš„é«˜æ•ˆã€ç²¾å‡†å®šä½ã€‚\n",
    "\n",
    "é˜¶æ®µ1ä¸ºå¾…æŸ¥æ–‡æ¡£ä¸»æ—¨ç›®æ ‡è§£æ„ï¼Œæ¨¡å‹è¢«æŒ‡ç¤ºä»æ–‡æœ¬ä¸­æç‚¼æ ¸å¿ƒäº‹ä»¶ã€å…³é”®æŠ€æœ¯ã€å·¥è‰ºæµç¨‹ç­‰è¦ç´ ï¼Œç»“æ„åŒ–åœ°æ€»ç»“æ–‡æ¡£çš„æ ¸å¿ƒå†…å®¹ï¼Œç”±æ­¤ç•Œå®šæœ¬æ¬¡å®¡æŸ¥çš„é¶å‘ç›®æ ‡ï¼Œä¸ºåç»­çš„ç²¾ç»†åŒ–é—®è¯¢å¥ å®šåŸºç¡€ã€‚é˜¶æ®µ2ä¸ºå¤šç»´åº¦é£é™©æ¢æµ‹ä¸å®šåˆ¶åŒ–é—®è¯¢ç”Ÿæˆï¼ŒåŸºäºç¬¬ä¸€é˜¶æ®µæç‚¼çš„æ ¸å¿ƒè¦ç´ ï¼Œé€šè¿‡few-shotç­‰æ–¹å¼å¼•å¯¼ LLM ä»åˆè§„æ€§ã€å®‰å…¨æ€§ã€å¯æ“ä½œæ€§ç­‰å¤šç»´åº¦å¯¹æ–‡æ¡£è¿›è¡Œé£é™©æ¢æµ‹ã€‚Prompt æŒ‡ç¤ºæ¨¡å‹å›´ç»•æ½œåœ¨çš„é™åˆ¶æ¡ä»¶ã€æ“ä½œæµç¨‹ã€ä»¥åŠå¯èƒ½å­˜åœ¨çš„åˆè§„æ€§éšæ‚£ç­‰æ–¹é¢ï¼Œè¿›è¡Œç»†ç²’åº¦ã€å¤šè§’åº¦çš„å®¡æŸ¥æé—®ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d283d9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "CORE_COMPONENTS_PROMPT = \"\"\"\n",
    "ä»»åŠ¡ï¼šä»ä¸‹é¢çš„å»ºç­‘æ–‡æ¡£ä¸­æå–å…³é”®ä¿¡æ¯ç»„ä»¶ã€‚é‡ç‚¹å…³æ³¨æŠ€æœ¯è¦æ±‚ã€æ–½å·¥æ–¹æ³•å’Œåˆè§„ç›¸å…³è¦ç´ ã€‚\n",
    "\n",
    "è¯·è¯†åˆ«ï¼š\n",
    "1. æŠ€æœ¯è§„æ ¼å’Œæ ‡å‡†\n",
    "2. æ–½å·¥æŠ€æœ¯å’Œå·¥è‰º\n",
    "3. è´¨é‡è¦æ±‚å’Œé™åˆ¶\n",
    "\n",
    "Input: {document_chunk}\n",
    "\n",
    "è¯·ç”¨ä¸­æ–‡æä¾›ç®€æ´æ€»ç»“ï¼š\n",
    "\"\"\"\n",
    "\n",
    "REVIEW_QUERIES_PROMPT = \"\"\"\n",
    "ä»»åŠ¡ï¼šåŸºäºå»ºç­‘æ–‡æ¡£å’Œæå–çš„ç»„ä»¶ç”Ÿæˆ3-5ä¸ªå…·ä½“çš„å®¡æŸ¥é—®é¢˜ã€‚è¿™äº›é—®é¢˜åº”å¸®åŠ©é€šè¿‡æ£€ç´¢ç›¸å…³å»ºç­‘è§„èŒƒå’Œæ ‡å‡†æ¥è¯†åˆ«æ½œåœ¨çš„åˆè§„æ€§é—®é¢˜ã€‚\n",
    "\n",
    "Document: {document_chunk}\n",
    "Key components: {core_components}\n",
    "\n",
    "ç”Ÿæˆå®¡æŸ¥é—®é¢˜ï¼ˆæ¯è¡Œä¸€ä¸ªé—®é¢˜ï¼‰ï¼š\n",
    "1.\n",
    "2.\n",
    "3.\n",
    "4.\n",
    "5.\n",
    "\"\"\"\n",
    "\n",
    "def generate_review_queries(llm, document_chunk: str) -> List[str]:\n",
    "    core_prompt = CORE_COMPONENTS_PROMPT.format(document_chunk=document_chunk)\n",
    "    core_response = llm.predict(core_prompt)\n",
    "    \n",
    "    queries_prompt = REVIEW_QUERIES_PROMPT.format(\n",
    "        document_chunk=document_chunk,\n",
    "        core_components=core_response\n",
    "    )\n",
    "    queries_response = llm.predict(queries_prompt)\n",
    "    \n",
    "    queries = []\n",
    "    lines = queries_response.strip().split('\\n')\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        line = re.sub(r'^\\d+[\\.\\)]\\s*', '', line)\n",
    "        line = re.sub(r'^\\*\\s*', '', line)\n",
    "        line = re.sub(r'^-\\s*', '', line) \n",
    "        \n",
    "        if line and len(line) > 5:\n",
    "            queries.append(line)\n",
    "    \n",
    "    return queries[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b93451",
   "metadata": {},
   "source": [
    "#### çŸ¥è¯†å¼•å¯¼ç”Ÿæˆå¼æ£€ç´¢\n",
    "\n",
    "ç³»ç»Ÿçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºçŸ¥è¯†å¼•å¯¼çš„æ£€ç´¢æ¡†æ¶ï¼Œæ•´ä¸ªè¿‡ç¨‹åˆ†ä¸ºä¸‰ä¸ªå…³é”®æ­¥éª¤ã€‚æ­¥éª¤1ä¸ºå¥å­çº§ç¼–ç ï¼Œä¸»è¦è´Ÿè´£è¾“å…¥æŸ¥è¯¢å¥å­çš„åˆå§‹è¡¨ç¤ºå­¦ä¹ ï¼Œè®¡ç®—æŸ¥è¯¢ä¸çŸ¥è¯†åº“chunksé—´çš„å¥å­çº§ç›¸ä¼¼åº¦åˆ†æ•°ã€‚æ­¥éª¤2ä¸ºçŸ¥è¯†å¼•å¯¼æ£€ç´¢ï¼Œè¿›ä¸€æ­¥ä»æŸ¥è¯¢ä¸­æå–å…³é”®ä¿¡æ¯ï¼Œåˆ©ç”¨è¿™äº›ä¿¡æ¯ç»“åˆæ–‡æ¡£é•¿åº¦è‡ªé€‚åº”åŠ æƒç­‰æœºåˆ¶ï¼Œå¯¹æ¯ä¸ªçŸ¥è¯†åº“chunkè¿›è¡Œæ›´è¯¦ç»†çš„è¯„åˆ†ã€‚æ­¥éª¤3ä¸ºé‡æ’åºä¸å¢å¼ºï¼Œä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹å¯¹æ­¥éª¤2æ£€ç´¢çš„ç»“æœè¿›è¡Œè¿›ä¸€æ­¥é‡æ’åºï¼Œå¹¶åˆ©ç”¨ç²¾ç‚¼çš„çŸ¥è¯†æ¥å¢å¼ºåŸå§‹æŸ¥è¯¢ã€‚\n",
    "![picture](images/pic2.png)\n",
    "\n",
    "é¦–å…ˆå»ºç«‹ä¸“é—¨é’ˆå¯¹å»ºç­‘é¢†åŸŸæ–‡æœ¬åˆ†æçš„æ·±åº¦æå–æ¨¡å—ï¼Œé›†æˆé¢†åŸŸé¢„è®­ç»ƒBERTè¿›è¡Œä¸Šä¸‹æ–‡ç¼–ç ï¼Œç»“åˆåŒå‘LSTMè¿›è¡Œå»ºç­‘æ³•è§„ä¾èµ–å»ºæ¨¡ã€‚å»ºç«‹ä¸‰çº§é‡è¦æ€§åˆ†ç±»å±‚æ¬¡ï¼šmaxï¼ˆæœ€é«˜ï¼‰ã€midï¼ˆä¸­ç­‰ï¼‰ã€litï¼ˆå­—é¢ï¼‰ä¼˜å…ˆçº§ã€‚æœ¬é¡¹ç›®ç›´æ¥é€šè¿‡å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œå…³é”®ä¿¡æ¯æå–ï¼Œå¦‚æœéœ€è¦æ›´ç²¾å‡†çš„æ•ˆæœï¼Œå¯ä»¥è‡ªè¡Œè®­ç»ƒBERTæ¨¡å‹è¿›è¡Œä¸“é—¨çš„å…³é”®ä¿¡æ¯æå–ã€‚\n",
    "![picture](images/pic3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc88609",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Dict, Tuple, List\n",
    "\n",
    "KEY_INFO_EXTRACTION_PROMPT = \"\"\"\n",
    "ä½ çš„ä»»åŠ¡æ˜¯ä»æŸ¥è¯¢ä¸­æå–å…³é”®ä¿¡æ¯ï¼Œåˆ†ä¸ºä¸‰ä¸ªä¸åŒçš„ä¼˜å…ˆçº§ï¼š\n",
    "\n",
    "æœ€é«˜ä¼˜å…ˆçº§ï¼ˆmaxï¼‰ï¼šæœ€é‡è¦çš„æ ¸å¿ƒæ¦‚å¿µæˆ–å®ä½“\n",
    "ä¸­ç­‰ä¼˜å…ˆçº§ï¼ˆmidï¼‰ï¼šé‡è¦çš„ä¿®é¥°è¯æˆ–é™å®šæ¡ä»¶\n",
    "å­—é¢ä¼˜å…ˆçº§ï¼ˆlitï¼‰ï¼šå…·ä½“æ•°å€¼ã€æ ‡å‡†æˆ–è§„æ ¼\n",
    "\n",
    "Query: {query}\n",
    "max:\n",
    "mid:\n",
    "lit:\n",
    "\"\"\"\n",
    "\n",
    "class KeyInfoExtractor:\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "\n",
    "    def extract_key_info(self, query: str) -> Dict[str, Tuple[str, float]]:\n",
    "        prompt = KEY_INFO_EXTRACTION_PROMPT.format(query=query)\n",
    "        response = self.llm.predict(prompt)\n",
    "        \n",
    "        lines = response.strip().split('\\n')\n",
    "        key_info = {}\n",
    "        weights = {'max': 0.5, 'mid': 0.3, 'lit': 0.2}\n",
    "        \n",
    "        for line in lines:\n",
    "            if line.startswith('max:'):\n",
    "                key_info['max'] = (line[4:].strip(), weights['max'])\n",
    "            elif line.startswith('mid:'):\n",
    "                key_info['mid'] = (line[4:].strip(), weights['mid'])\n",
    "            elif line.startswith('lit:'):\n",
    "                key_info['lit'] = (line[4:].strip(), weights['lit'])\n",
    "        \n",
    "        return key_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322faa01",
   "metadata": {},
   "source": [
    "#### æ–‡æ¡£é•¿åº¦è‡ªé€‚åº”å› å­\n",
    "\n",
    "åœ¨çŸ¥è¯†å¼•å¯¼æ£€ç´¢è¿‡ç¨‹ä¸­ï¼Œæ–‡æ¡£é•¿åº¦è‡ªé€‚åº”å› å­ç”¨äºè°ƒæ•´ä¸åŒé•¿åº¦æ–‡æ¡£çš„æƒé‡åˆ†é…ï¼Œç¡®ä¿é•¿çŸ­æ–‡æ¡£éƒ½èƒ½å¾—åˆ°å…¬å¹³çš„è¯„åˆ†æœºä¼šã€‚è¯¥å› å­çš„è®¡ç®—è€ƒè™‘äº†å½“å‰æ–‡æ¡£chunkçš„é•¿åº¦ä¸å¹³å‡æ–‡æ¡£é•¿åº¦çš„å…³ç³»ã€‚\n",
    "\n",
    "$$\\Lambda_{\\text{DL}} = \\frac{\\overline{|k|} + |k_j|}{2\\overline{|k|}}$$\n",
    "\n",
    "å…¶ä¸­ $|k_j|$ è¡¨ç¤ºå½“å‰æ–‡æ¡£chunkçš„é•¿åº¦ï¼Œ$\\overline{|k|}$ è¡¨ç¤ºå¹³å‡æ–‡æ¡£é•¿åº¦ã€‚é€šè¿‡è¿™ç§å½’ä¸€åŒ–å¤„ç†ï¼Œå¯ä»¥é¿å…å› æ–‡æ¡£é•¿åº¦å·®å¼‚å¯¼è‡´çš„è¯„åˆ†åå·®ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290be1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_document_length_factor(chunk_length: int, avg_length: int = 100) -> float:\n",
    "    lambda_dl = (avg_length + chunk_length) / (2 * avg_length)\n",
    "    return lambda_dl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8602cb08",
   "metadata": {},
   "source": [
    "#### æœ¯è¯­é‡è¦æ€§è®¡ç®—\n",
    "\n",
    "æœ¯è¯­é‡è¦æ€§æŒ‡æ ‡è¡¡é‡æœ¯è¯­åœ¨æ–‡æ¡£ä¸­çš„æ˜¾è‘—ç¨‹åº¦ï¼Œç»“åˆæœ¯è¯­é¢‘ç‡å’Œæ–‡æ¡£é•¿åº¦è‡ªé€‚åº”å› å­ï¼Œèƒ½å¤Ÿæ›´å‡†ç¡®åœ°è¯„ä¼°æœ¯è¯­åœ¨å½“å‰æ–‡æ¡£ä¸­çš„é‡è¦æ€§ã€‚è®¡ç®—å…¬å¼è€ƒè™‘äº†æœ¯è¯­é¢‘ç‡çš„éçº¿æ€§å¢é•¿ç‰¹æ€§ã€‚\n",
    "\n",
    "$$\\text{Sign}(t_{e_i}^\\tau, k_j) = \\frac{2 \\cdot f(t_{e_i}^\\tau, k_j) \\cdot \\Lambda_{\\text{DL}}}{f(t_{e_i}^\\tau, k_j) + 1}$$\n",
    "\n",
    "å…¶ä¸­ $f(t_{e_i}^\\tau, k_j)$ è¡¨ç¤ºæœ¯è¯­åœ¨æ–‡æ¡£chunkä¸­çš„å‡ºç°é¢‘ç‡ï¼Œ$\\Lambda_{\\text{DL}}$ ä¸ºæ–‡æ¡£é•¿åº¦è‡ªé€‚åº”å› å­ã€‚è¿™ç§è®¡ç®—æ–¹å¼èƒ½å¤Ÿé˜²æ­¢é«˜é¢‘æœ¯è¯­è¿‡åº¦å½±å“è¯„åˆ†ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8bcc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_term_significance(term_freq: int, doc_length_factor: float) -> float:\n",
    "    significance = (2 * term_freq * doc_length_factor) / (term_freq + 1)\n",
    "    return significance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bf4a3b",
   "metadata": {},
   "source": [
    "#### æœ¯è¯­ç¨€æœ‰åº¦è®¡ç®—\n",
    "\n",
    "æœ¯è¯­ç¨€æœ‰åº¦ç”¨äºè¡¡é‡æœ¯è¯­åœ¨æ•´ä¸ªçŸ¥è¯†åº“ä¸­çš„ç¨€ç¼ºç¨‹åº¦ï¼Œç¨€æœ‰åº¦è¶Šé«˜çš„æœ¯è¯­åœ¨æ£€ç´¢ä¸­çš„æƒé‡è¶Šå¤§ã€‚è®¡ç®—é‡‡ç”¨äº†æ”¹è¿›çš„IDFå…¬å¼ï¼Œå¢åŠ äº†å¹³æ»‘å¤„ç†ä»¥é¿å…é›¶é™¤é—®é¢˜ã€‚\n",
    "\n",
    "$\\text{Rarity}(t_{e_i}^\\tau) = \\log\\left(\\frac{D - \\text{df}(t_{e_i}^\\tau) + 0.5}{\\text{df}(t_{e_i}^\\tau) + 0.5} + 1\\right)$\n",
    "\n",
    "å…¶ä¸­ $D$ è¡¨ç¤ºæ–‡æ¡£æ€»æ•°ï¼Œ$\\text{df}(t_{e_i}^\\tau)$ è¡¨ç¤ºåŒ…å«è¯¥æœ¯è¯­çš„æ–‡æ¡£æ•°é‡ã€‚åŠ ä¸€æ“ä½œç¡®ä¿äº†å¯¹æ•°å€¼å§‹ç»ˆä¸ºæ­£æ•°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dc2700",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_term_rarity(doc_freq: int, total_docs: int) -> float:\n",
    "    rarity = np.log((total_docs - doc_freq + 0.5) / (doc_freq + 0.5) + 1)\n",
    "    return rarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19ec6e7",
   "metadata": {},
   "source": [
    "#### è¿è´¯æ€§æŒ‡æ•°è¯„ä¼°\n",
    "\n",
    "è¿è´¯æ€§æŒ‡æ•°åæ˜ æœ¯è¯­åœ¨æ–‡æ¡£ä¸­çš„åˆ†å¸ƒè¿è´¯æ€§ï¼Œé€šè¿‡æ»‘åŠ¨çª—å£æŠ€æœ¯åˆ†ææœ¯è¯­åœ¨æ–‡æ¡£ä¸­çš„å±€éƒ¨åˆ†å¸ƒæƒ…å†µã€‚è¿è´¯æ€§é«˜çš„æœ¯è¯­å¾€å¾€åœ¨æ–‡æ¡£çš„ç‰¹å®šåŒºåŸŸé›†ä¸­å‡ºç°ï¼Œè¡¨æ˜å…¶ä¸æ–‡æ¡£ä¸»é¢˜çš„å¼ºç›¸å…³æ€§ã€‚\n",
    "\n",
    "$$\\text{CI}(t_{e_i}^\\tau, k_j) = \\max_{w \\in W, \\, t \\in w} \\frac{\\sum I(t = t_{e_i}^\\tau) \\cdot |w|}{|k_j|}$$\n",
    "\n",
    "å…¶ä¸­ $W$ è¡¨ç¤ºæ–‡æ¡£ä¸­çš„æ»‘åŠ¨çª—å£é›†åˆï¼Œ$I(t = t_{e_i}^\\tau)$ ä¸ºæŒ‡ç¤ºå‡½æ•°ï¼Œå½“çª—å£ä¸­åŒ…å«è¯¥æœ¯è¯­æ—¶ä¸º1ï¼Œå¦åˆ™ä¸º0ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33644f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_index(term: str, chunk: str, window_size: int = 50) -> float:\n",
    "    chunk_tokens = chunk.lower().split()\n",
    "    chunk_length = len(chunk_tokens)\n",
    "    \n",
    "    if chunk_length == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    max_coherence = 0.0\n",
    "    \n",
    "    for i in range(0, chunk_length - window_size + 1, 10):\n",
    "        window = chunk_tokens[i:i + window_size]\n",
    "        term_count = window.count(term.lower())\n",
    "        \n",
    "        if term_count > 0:\n",
    "            coherence = (term_count * window_size) / chunk_length\n",
    "            max_coherence = max(max_coherence, coherence)\n",
    "    \n",
    "    return max_coherence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494fe967",
   "metadata": {},
   "source": [
    "#### è¯„åˆ†èåˆä¸æ£€ç´¢\n",
    "\n",
    "å°†å¥å­çº§ç›¸ä¼¼åº¦è¯„åˆ†ä¸çŸ¥è¯†çº§è¯„åˆ†è¿›è¡Œèåˆï¼Œå½¢æˆæœ€ç»ˆçš„æ–‡æ¡£ç›¸å…³æ€§è¯„åˆ†ã€‚èåˆè¿‡ç¨‹é‡‡ç”¨åŠ æƒå¹³å‡çš„æ–¹å¼ï¼Œå¹³è¡¡å‚æ•°Î»æ§åˆ¶ä¸¤ç§è¯„åˆ†æ–¹å¼çš„é‡è¦æ€§ã€‚\n",
    "\n",
    "$\\Phi = \\lambda \\Phi(\\mathcal{K}) + (1 - \\lambda) \\Phi(\\mathcal{S})$\n",
    "\n",
    "å…¶ä¸­ $\\lambda$ ä¸ºå¹³è¡¡å‚æ•°ï¼Œ$\\Phi(\\mathcal{K})$ ä¸ºçŸ¥è¯†çº§è¯„åˆ†ï¼Œ$\\Phi(\\mathcal{S})$ ä¸ºå¥å­çº§è¯„åˆ†ã€‚é€šè¿‡è°ƒæ•´Î»å€¼ï¼Œå¯ä»¥æ§åˆ¶ç³»ç»Ÿæ›´åå‘è¯­ä¹‰ç›¸ä¼¼è¿˜æ˜¯çŸ¥è¯†åŒ¹é…ã€‚å½“Î»=0æ—¶ï¼Œç³»ç»Ÿå®Œå…¨ä¾èµ–å¥å­çº§è¯­ä¹‰ç›¸ä¼¼åº¦ï¼›å½“Î»=1æ—¶ï¼Œç³»ç»Ÿå®Œå…¨ä¾èµ–çŸ¥è¯†åŒ¹é…è¯„åˆ†ï¼›Î»=0.5æ—¶ï¼Œä¸¤ç§è¯„åˆ†æ–¹å¼æƒé‡ç›¸ç­‰ã€‚åœ¨å»ºç­‘æ–‡æ¡£å®¡æŸ¥åœºæ™¯ä¸­ï¼Œé€šå¸¸è®¾ç½®Î»=0.5ä»¥å¹³è¡¡ä¸“ä¸šçŸ¥è¯†åŒ¹é…å’Œè¯­ä¹‰ç†è§£ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d44da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "class GKGRRetriever:\n",
    "    def __init__(self, \n",
    "                 knowledge_base: List[str],\n",
    "                 embedding_model,\n",
    "                 key_info_extractor: KeyInfoExtractor,\n",
    "                 llm,\n",
    "                 config: Dict[str, Any] = None):\n",
    "        self.knowledge_base = knowledge_base\n",
    "        self.embedding_model = embedding_model\n",
    "        self.key_info_extractor = key_info_extractor\n",
    "        self.llm = llm\n",
    "        \n",
    "        default_config = {\n",
    "            \"lambda_param\": 0.5,\n",
    "            \"top_k\": 5,\n",
    "            \"rerank_enabled\": True,\n",
    "            \"query_expansion\": True,\n",
    "            \"similarity_threshold\": 0.1\n",
    "        }\n",
    "        self.config = {**default_config, **(config or {})}\n",
    "        \n",
    "        self.kb_embeddings = self._precompute_embeddings()\n",
    "    \n",
    "    def _precompute_embeddings(self) -> np.ndarray:\n",
    "        embeddings = self.embedding_model.encode(self.knowledge_base, show_progress_bar=True)\n",
    "        return embeddings\n",
    "    \n",
    "    def retrieve_with_scores(self, query: str) -> List[Tuple[str, float, Dict[str, float]]]:\n",
    "        query_embedding = self.embedding_model.encode([query])[0]\n",
    "        sentence_scores = cosine_similarity(\n",
    "            query_embedding.reshape(1, -1), \n",
    "            self.kb_embeddings\n",
    "        )[0]\n",
    "        \n",
    "        key_info = self.key_info_extractor.extract_key_info(query)\n",
    "        knowledge_scores = self._compute_knowledge_scores(key_info)\n",
    "        \n",
    "        final_scores = []\n",
    "        for i in range(len(self.knowledge_base)):\n",
    "            norm_sent = sentence_scores[i]\n",
    "            norm_know = knowledge_scores[i] / max(knowledge_scores) if max(knowledge_scores) > 0 else 0\n",
    "            \n",
    "            final_score = (self.config[\"lambda_param\"] * norm_know + \n",
    "                          (1 - self.config[\"lambda_param\"]) * norm_sent)\n",
    "            final_scores.append(final_score)\n",
    "        \n",
    "        results_with_scores = []\n",
    "        for i, final_score in enumerate(final_scores):\n",
    "            if final_score > self.config[\"similarity_threshold\"]:\n",
    "                score_details = {\n",
    "                    \"sentence_score\": float(sentence_scores[i]),\n",
    "                    \"knowledge_score\": float(knowledge_scores[i]),\n",
    "                    \"final_score\": float(final_score)\n",
    "                }\n",
    "                results_with_scores.append((self.knowledge_base[i], final_score, score_details))\n",
    "        \n",
    "        results_with_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        return results_with_scores[:self.config[\"top_k\"]]\n",
    "    \n",
    "    def _compute_knowledge_scores(self, key_info: Dict[str, Tuple[str, float]]) -> List[float]:\n",
    "        scores = []\n",
    "        avg_length = sum(len(chunk.split()) for chunk in self.knowledge_base) / len(self.knowledge_base)\n",
    "        \n",
    "        for chunk in self.knowledge_base:\n",
    "            chunk_score = 0.0\n",
    "            chunk_tokens = chunk.lower().split()\n",
    "            chunk_length = len(chunk_tokens)\n",
    "            \n",
    "            lambda_dl = compute_document_length_factor(chunk_length, avg_length)\n",
    "            \n",
    "            for priority, (info_text, weight) in key_info.items():\n",
    "                if not info_text.strip():\n",
    "                    continue\n",
    "                \n",
    "                terms = info_text.lower().split()\n",
    "                for term in terms:\n",
    "                    if term in chunk_tokens:\n",
    "                        tf = chunk_tokens.count(term)\n",
    "                        \n",
    "                        significance = compute_term_significance(tf, lambda_dl)\n",
    "                        \n",
    "                        segments_with_term = sum(1 for kb_chunk in self.knowledge_base \n",
    "                                                if term in kb_chunk.lower())\n",
    "                        rarity = compute_term_rarity(segments_with_term, len(self.knowledge_base))\n",
    "                        \n",
    "                        coherence = compute_coherence_index(term, chunk)\n",
    "                        \n",
    "                        term_score = significance * rarity * (1 + coherence) * weight\n",
    "                        chunk_score += term_score\n",
    "            \n",
    "            scores.append(chunk_score)\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    def retrieve(self, query: str) -> Tuple[List[str], str]:\n",
    "        results_with_scores = self.retrieve_with_scores(query)\n",
    "        \n",
    "        documents = [doc for doc, _, _ in results_with_scores]\n",
    "        \n",
    "        if self.config[\"rerank_enabled\"] and len(documents) > 1:\n",
    "            documents = self._llm_rerank(query, documents)\n",
    "        \n",
    "        augmented_query = query\n",
    "        if self.config[\"query_expansion\"]:\n",
    "            augmented_query = self._augment_query(query, documents[:3])\n",
    "        \n",
    "        return documents, augmented_query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23e7375",
   "metadata": {},
   "source": [
    "#### é‡æ’åºä¼˜åŒ–\n",
    "\n",
    "ç³»ç»Ÿä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹å¯¹æ£€ç´¢ç»“æœè¿›è¡Œè¿›ä¸€æ­¥é‡æ’åºï¼Œé€šè¿‡LLMçš„è¯­ä¹‰ç†è§£èƒ½åŠ›ä¼˜åŒ–æ–‡æ¡£çš„ç›¸å…³æ€§æ’åºã€‚é‡æ’åºè¿‡ç¨‹ä¸­ï¼Œç³»ç»Ÿä¼šæ„é€ åŒ…å«æŸ¥è¯¢å’Œå€™é€‰æ–‡æ¡£çš„æç¤ºï¼Œè¦æ±‚LLMæ ¹æ®ç›¸å…³æ€§å¯¹æ–‡æ¡£è¿›è¡Œé‡æ–°æ’åºã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae7c7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _llm_rerank(self, query: str, documents: List[str]) -> List[str]:\n",
    "    if len(documents) <= 1:\n",
    "        return documents\n",
    "    \n",
    "    rerank_prompt = f\"\"\"\n",
    "ä»»åŠ¡ï¼šæ–‡æ¡£åˆ—è¡¨å¦‚ä¸‹æ‰€ç¤ºã€‚æ¯ä¸ªæ–‡æ¡£æ—è¾¹éƒ½æœ‰ä¸€ä¸ªæ•°å­—ã€‚è¿˜æä¾›äº†ä¸€ä¸ªé—®é¢˜ã€‚æ‚¨çš„ä»»åŠ¡æ˜¯æŒ‰ç…§ç›¸å…³æ€§ä»æœ€ç›¸å…³åˆ°æœ€ä¸ç›¸å…³çš„é¡ºåºè¿”å›æ‰€æœ‰æ–‡æ¡£çš„ç¼–å·ã€‚å¿…é¡»åŒ…å«æ¯ä¸ªæ–‡æ¡£å·ä¸€æ¬¡ã€‚\n",
    "\n",
    "è¾“å‡ºæ ·ä¾‹:\n",
    "    Document 1: <document 1>\n",
    "    Document 2: <document 2>\n",
    "    Document 3: <document 3>\n",
    "    é—®é¢˜: <question>\n",
    "    å›ç­”: 3,1,2\n",
    "\n",
    "Now here are the actual documents and question.\n",
    "\n",
    "\"\"\"\n",
    "    for i, doc in enumerate(documents):\n",
    "        rerank_prompt += f\"Document {i+1}: {doc[:150]}...\\n\"\n",
    "    \n",
    "    rerank_prompt += f\"Question: {query}\\nAnswer:\"\n",
    "    \n",
    "    try:\n",
    "        response = self.llm.predict(rerank_prompt)\n",
    "        order_nums = [int(x.strip()) - 1 for x in response.split(',') \n",
    "                     if x.strip().isdigit() and 0 <= int(x.strip()) - 1 < len(documents)]\n",
    "        \n",
    "        reranked = [documents[i] for i in order_nums if i < len(documents)]\n",
    "        \n",
    "        # æ·»åŠ é—æ¼çš„æ–‡æ¡£\n",
    "        used_indices = set(order_nums)\n",
    "        for i, doc in enumerate(documents):\n",
    "            if i not in used_indices:\n",
    "                reranked.append(doc)\n",
    "        \n",
    "        return reranked[:len(documents)]\n",
    "    except:\n",
    "        return documents\n",
    "    \n",
    "GKGRRetriever._llm_rerank = _llm_rerank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ee71c1",
   "metadata": {},
   "source": [
    "#### æŸ¥è¯¢å¢å¼º\n",
    "\n",
    "åŒæ—¶ç³»ç»Ÿè¿˜ä¼šåˆ©ç”¨æ£€ç´¢åˆ°çš„çŸ¥è¯†æ¥å¢å¼ºåŸå§‹æŸ¥è¯¢ï¼Œç”Ÿæˆæ›´å…·ä½“ã€æ›´è¯¦ç»†çš„æŸ¥è¯¢ç”¨äºè¿›ä¸€æ­¥æ£€ç´¢ã€‚æŸ¥è¯¢å¢å¼ºé€šè¿‡åˆ†ææ£€ç´¢ç»“æœçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè¯†åˆ«æŸ¥è¯¢ä¸­å¯èƒ½é—æ¼çš„å…³é”®æ¦‚å¿µå’Œæœ¯è¯­ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6e9451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _augment_query(self, original_query: str, top_results: List[str]) -> str:\n",
    "    if not top_results:\n",
    "        return original_query\n",
    "    \n",
    "    document_list = \"\"\n",
    "    for i, doc in enumerate(top_results):\n",
    "        document_list += f\"Document {i+1}: {doc[:100]}...\\n\"\n",
    "    \n",
    "    augment_prompt = f\"\"\"\n",
    "ä»»åŠ¡ï¼šæ‚¨çš„ä»»åŠ¡æ˜¯é€šè¿‡ç»¼åˆæ‰€æœ‰æä¾›çš„æ–‡æ¡£ä¸­çš„ä¿¡æ¯æ¥ç”Ÿæˆé—®é¢˜çš„è¯¦ç»†ç­”æ¡ˆã€‚ä¼˜å…ˆè€ƒè™‘ç›¸å…³æ€§ï¼Œå¼•ç”¨æ–‡æ¡£ç¼–å·ï¼Œå¹¶ä½¿ç”¨ä¸­æ–‡ï¼ŒæŒ‰ç…§ä»¥ä¸‹æ–¹å¼ç»„ç»‡ä½ çš„å›å¤ï¼š\n",
    "\n",
    "Question: {original_query}\n",
    "{document_list}\n",
    "Answer:\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        augmented = self.llm.predict(augment_prompt)\n",
    "        return augmented.strip()\n",
    "    except:\n",
    "        return original_query\n",
    "\n",
    "GKGRRetriever._augment_query = _augment_query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c783319",
   "metadata": {},
   "source": [
    "#### åå·®æ£€æµ‹åˆ†æ\n",
    "\n",
    "åœ¨å…ˆæœŸçŸ¥è¯†å¢å¼ºæ£€ç´¢é˜¶æ®µè·å–é¢†åŸŸçŸ¥è¯†åï¼Œç³»ç»Ÿéšå³è¿›å…¥è¯¯å·®è¾¨ææ¨¡å—ã€‚è¯¥æ¨¡å—åŸºäºæ£€ç´¢å¾—åˆ°çš„çŸ¥è¯†å‚è€ƒï¼Œå¹¶ç»“åˆé¢„è®¾çš„å®¡é˜…é—®é¢˜ï¼Œå¯¹åŸæ–‡è¿›è¡Œç»†è‡´çš„åå·®æ£€æµ‹ä¸è¯„ä¼°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d84e7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ErrorAnalyzer:\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "    \n",
    "    def analyze_errors(self, document_chunk: str, query: str, retrieved_knowledge: List[str]) -> Dict[str, Any]:\n",
    "        \n",
    "        analysis_prompt = f\"\"\"\n",
    "ä»»åŠ¡ï¼šä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®æä¾›çš„å®¡æŸ¥æŸ¥è¯¢å’Œç›¸å…³å‚è€ƒè§„èŒƒï¼Œå¯¹ç»™å®šçš„å®¡æŸ¥æ–‡æ¡£è¿›è¡Œé”™è¯¯åˆ†æã€‚æ­¤åˆ†æå¿…é¡»ä¸¥æ ¼éµå®ˆæ‰€æä¾›çš„å‚è€ƒèµ„æ–™ï¼Œå¹¶ç‰¹åˆ«æ³¨é‡å®¡æŸ¥å’Œåˆ†æå®¡æŸ¥æ–‡ä»¶ä¸­çš„åŸå§‹æè¿°éƒ¨åˆ†ã€‚æœ€åä½¿ç”¨ä¸­æ–‡è¾“å‡ºã€‚\n",
    "\n",
    "Review document: {document_chunk}\n",
    "Query: {query}\n",
    "Reference: {chr(10).join([f\"{i+1}. {ref}\" for i, ref in enumerate(retrieved_knowledge)])}\n",
    "Analysis:\n",
    "\"\"\"\n",
    "        \n",
    "        analysis = self.llm.predict(analysis_prompt)\n",
    "        \n",
    "        return {\n",
    "            \"analysis\": analysis,\n",
    "            \"reference_support\": retrieved_knowledge\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9421c272",
   "metadata": {},
   "source": [
    "#### ä¿®è®¢å»ºè®®ç”Ÿæˆ\n",
    "\n",
    "è¯¯å·®è¾¨ææ¨¡å—å®Œæˆåï¼Œç³»ç»Ÿå°†è¾“å‡ºæ ‡è®°åå·®åŒºåŸŸä»¥åŠç›¸å…³çŸ¥è¯†ä½è¯ã€‚éšåï¼Œç³»ç»Ÿè¿›å…¥ä¿®è®¢ç­–ç•¥ç”Ÿæˆæ¨¡å—ã€‚è¯¥æ¨¡å—ä¾æ®è¯¯å·®åˆ†æç»“æœå’ŒçŸ¥è¯†å‚è€ƒï¼Œå¯¹æ ‡è®°åŒºåŸŸè¿›è¡Œé’ˆå¯¹æ€§çš„ä¿®è®¢å»ºè®®ç”Ÿæˆï¼Œæœ€ç»ˆå®ç°å¯¹åŸæ–‡çš„çŸ¥è¯†é©±åŠ¨å‹è‡ªåŠ¨ä¿®æ­£ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4ed81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RevisionGenerator:\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "    \n",
    "    def generate_revisions(self, document_chunk: str, analysis: Dict[str, Any]) -> Dict[str, str]:     \n",
    "        revision_prompt = f\"\"\"\n",
    "ä»»åŠ¡ï¼šä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ç»™å®šçš„åˆ†æå’Œç›¸åº”çš„å‚è€ƒè§„èŒƒå®¡æŸ¥å’Œä¿®æ”¹æ‰€æä¾›çš„æ–‡ä»¶ã€‚è¦æ±‚ä¸¥æ ¼éµå®ˆæ‰€æä¾›çš„å‚è€ƒè§„èŒƒã€‚å¦‚æœè¯„å®¡æ–‡ä»¶ä¸åˆ†æå’Œå‚è€ƒè§„èŒƒä¸€è‡´ï¼Œæ²¡æœ‰å·®å¼‚ï¼Œåˆ™ä¸éœ€è¦ä¿®è®¢ã€‚æœ€åä½¿ç”¨ä¸­æ–‡è¾“å‡ºã€‚\n",
    "\n",
    "Review document: {document_chunk}\n",
    "Analysis: {analysis['analysis']}\n",
    "Reference: {chr(10).join([f\"- {ref}\" for ref in analysis['reference_support']])}\n",
    "Revision:\n",
    "\"\"\"\n",
    "        \n",
    "        revision = self.llm.predict(revision_prompt)\n",
    "        \n",
    "        return {\n",
    "            \"original_text\": document_chunk,\n",
    "            \"revision_suggestions\": revision,\n",
    "            \"modified_regions\": analysis.get(\"error_regions\", []),\n",
    "            \"confidence\": self._calculate_confidence(analysis)\n",
    "        }\n",
    "    \n",
    "    def _calculate_confidence(self, analysis: Dict[str, Any]) -> float:\n",
    "        ref_count = len(analysis.get(\"reference_support\", []))\n",
    "        error_count = len(analysis.get(\"error_regions\", []))\n",
    "        \n",
    "        confidence = min(0.9, 0.5 + (ref_count * 0.1) + (error_count * 0.05))\n",
    "        return confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef2f9f5",
   "metadata": {},
   "source": [
    "#### å®Œæ•´å®¡æŸ¥æµç¨‹\n",
    "\n",
    "å°†ä¸Šè¿°æ‰€æœ‰æ¨¡å—æ•´åˆï¼Œå½¢æˆå®Œæ•´çš„æ–‡æ¡£å®¡æŸ¥æµç¨‹ã€‚ç³»ç»Ÿé¦–å…ˆç”Ÿæˆå®¡æŸ¥é—®é¢˜ï¼Œç„¶åè¿›è¡ŒçŸ¥è¯†å¼•å¯¼æ£€ç´¢ï¼Œæ¥ç€æ‰§è¡Œé”™è¯¯åˆ†æï¼Œæœ€åç”Ÿæˆä¿®è®¢å»ºè®®ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7378cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_review_process(document_chunk: str, \n",
    "                          gkgr_framework: GKGRRetriever, \n",
    "                          error_analyzer: ErrorAnalyzer,\n",
    "                          revision_generator: RevisionGenerator) -> Dict[str, Any]:    \n",
    "    review_queries = generate_review_queries(gkgr_framework.llm, document_chunk)\n",
    "    \n",
    "    results = {}\n",
    "    for query in review_queries[:3]:\n",
    "        retrieved_docs, augmented_query = gkgr_framework.retrieve(query)\n",
    "        \n",
    "        knowledge_refs = retrieved_docs\n",
    "        analysis = error_analyzer.analyze_errors(document_chunk, query, knowledge_refs)\n",
    "        \n",
    "        revision = revision_generator.generate_revisions(document_chunk, analysis)\n",
    "        \n",
    "        results[query] = {\n",
    "            \"retrieved_knowledge\": retrieved_docs,\n",
    "            \"augmented_query\": augmented_query,\n",
    "            \"analysis\": analysis,\n",
    "            \"revision\": revision\n",
    "        }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc6173a",
   "metadata": {},
   "source": [
    "è‡³æ­¤ï¼Œæˆ‘ä»¬å°±å®Œæˆäº†å»ºç­‘æ–‡æ¡£æ™ºèƒ½å®¡æŸ¥ç³»ç»Ÿçš„æ ¸å¿ƒå®ç°ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326ff27c",
   "metadata": {},
   "source": [
    "## å®é™…åº”ç”¨ç¤ºä¾‹\n",
    "\n",
    "è®©æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªå®Œæ•´çš„ç¤ºä¾‹æ¥å±•ç¤ºç³»ç»Ÿçš„ä½¿ç”¨ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60cd7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ¯ ä¿®å¤åçš„GKGRå»ºç­‘æ–‡æ¡£å®¡æŸ¥ç³»ç»Ÿæµ‹è¯•\n",
      "================================================================================\n",
      "\n",
      "ğŸ“¦ Step 1: ä½¿ç”¨å·²åˆå§‹åŒ–çš„ç»„ä»¶...\n",
      "âœ… LLM: llama3.1:8b (Ollama)\n",
      "âœ… Embedding: BAAI/bge-m3 (Windowsä¼˜åŒ–ç‰ˆ)\n",
      "âš ï¸ æ³¨æ„ï¼šä½¿ç”¨è™šæ‹Ÿembeddingæ¨¡å¼ï¼ˆä»…ç”¨äºæ¶æ„æµ‹è¯•ï¼‰\n",
      "\n",
      "ğŸ“¦ Step 2: åˆ›å»ºç®€å•å»ºç­‘çŸ¥è¯†åº“...\n",
      "ğŸ”„ ä½¿ç”¨ä¿®å¤åçš„åŠ¨æ€è¯­ä¹‰åˆ†å—å™¨å¤„ç†æ–‡æ¡£...\n",
      "âœ… çŸ¥è¯†åº“æ„å»ºå®Œæˆï¼Œå…±5ä¸ªchunks\n",
      "  Chunk 1: é’¢ç­‹æ··å‡åœŸæŸ±çš„æ–½å·¥è§„èŒƒ æ··å‡åœŸå¼ºåº¦ç­‰çº§ä¸åº”ä½äºC25 é’¢ç­‹ä¿æŠ¤å±‚åšåº¦åº”ç¬¦åˆè®¾è®¡è¦æ±‚ æŸ±å­çš„å‚ç›´åº¦åå·®...\n",
      "  Chunk 2: æ··å‡åœŸæµ‡ç­‘çš„æŠ€æœ¯è¦æ±‚ æµ‡ç­‘åº”è¿ç»­è¿›è¡Œ æµ‡ç­‘é—´æ­‡æ—¶é—´ä¸åº”è¶…è¿‡æ··å‡åœŸçš„åˆå‡æ—¶é—´ æ¯å±‚æµ‡ç­‘åšåº¦ä¸å®œè¶…è¿‡æŒ¯æ£...\n",
      "  Chunk 3: æ··å‡åœŸå…»æŠ¤è§„èŒƒ å…»æŠ¤æœŸé—´åº”ä¿æŒæ··å‡åœŸè¡¨é¢æ¹¿æ¶¦ å…»æŠ¤æ—¶é—´ä¸å°‘äº7å¤© å¯¹äºé‡è¦ç»“æ„ï¼Œå…»æŠ¤æ—¶é—´åº”å»¶é•¿è‡³14...\n",
      "\n",
      "ğŸ“¦ Step 3: åˆå§‹åŒ–é«˜çº§RAGç»„ä»¶...\n",
      "âœ… KeyInfoExtractoråˆå§‹åŒ–å®Œæˆ\n",
      "âœ… GKGRRetrieveråˆå§‹åŒ–å®Œæˆ\n",
      "âœ… ErrorAnalyzeråˆå§‹åŒ–å®Œæˆ\n",
      "âœ… RevisionGeneratoråˆå§‹åŒ–å®Œæˆ\n",
      "\n",
      "ğŸ“¦ Step 4: æµ‹è¯•å®Œæ•´å®¡æŸ¥æµç¨‹...\n",
      "ğŸ“„ å¾…å®¡æŸ¥æ–‡æ¡£ï¼š\n",
      "é’¢ç­‹æ··å‡åœŸæŸ±çš„æ–½å·¥åº”ç¬¦åˆä»¥ä¸‹è¦æ±‚ï¼š\n",
      "1. æ··å‡åœŸå¼ºåº¦ç­‰çº§ä¸ä½äºC25\n",
      "2. é’¢ç­‹ä¿æŠ¤å±‚åšåº¦ä¸º25mm\n",
      "3. æ··å‡åœŸæµ‡ç­‘åº”è¿ç»­è¿›è¡Œï¼Œé—´æ­‡æ—¶é—´ä¸è¶…è¿‡1å°æ—¶\n",
      "4. å…»æŠ¤æœŸé—´åº”ä¿æŒæ··å‡åœŸè¡¨é¢æ¹¿æ¶¦\n",
      "\n",
      "ğŸ”„ æ‰§è¡Œå®Œæ•´å®¡æŸ¥æµç¨‹...\n",
      "\n",
      "================================================================================\n",
      "ğŸ“‹ å®¡æŸ¥ç»“æœ\n",
      "================================================================================\n",
      "\n",
      "â“ å®¡æŸ¥é—®é¢˜: åŸºäºç»™å‡ºçš„å»ºç­‘æ–‡æ¡£å’Œå…³é”®ä¿¡æ¯ç»„ä»¶ï¼Œç”Ÿæˆä»¥ä¸‹å…·ä½“çš„å®¡æŸ¥é—®é¢˜ï¼š\n",
      "ğŸ“š æ£€ç´¢åˆ°çš„çŸ¥è¯†: 5ä¸ªç›¸å…³æ–‡æ¡£\n",
      "ğŸ” å¢å¼ºæŸ¥è¯¢: åŸºäºæä¾›çš„æ–‡æ¡£å’Œå…³é”®ä¿¡æ¯ç»„ä»¶ï¼Œä»¥ä¸‹æ˜¯å®¡æŸ¥é—®é¢˜ï¼š\n",
      "\n",
      "1. **æ··å‡åœŸå¼ºåº¦ç­‰çº§**ï¼šè¯·ç¡®è®¤é’¢ç­‹æ··å‡åœŸæŸ±çš„æ–½å·¥è§„èŒƒä¸­æ··å‡åœŸå¼ºåº¦ç­‰çº§æ˜¯å¦ç¬¦åˆè§„å®šï¼ˆä¸åº”ä½äºC25ï¼‰ã€‚ï¼ˆå‚è€ƒ Document 1ï¼‰\n",
      "\n",
      "2. **...\n",
      "âš ï¸ é”™è¯¯åˆ†æ: å®¡æŸ¥æ–‡æ¡£ä¸­å…³äºé’¢ç­‹æ··å‡åœŸæŸ±æ–½å·¥çš„è¦æ±‚ä¸æä¾›çš„å‚è€ƒè§„èŒƒç›¸æ¯”ï¼Œæœ‰ä¸€äº›ç»†å¾®å·®åˆ«ã€‚å…·ä½“åˆ†æå¦‚ä¸‹ï¼š\n",
      "\n",
      "1. æ··å‡åœŸå¼ºåº¦ç­‰çº§ä¸ä½äºC25ï¼šè¿™ä¸ªè¦æ±‚åœ¨ä¸¤ä¸ªèµ„æ–™ä¸­éƒ½æœ‰æåˆ°ï¼Œå‡ä¸ºç›¸åŒçš„æ ‡å‡†ã€‚\n",
      "2. é’¢ç­‹ä¿æŠ¤å±‚åšåº¦ä¸º25mmï¼šåœ¨å®¡æŸ¥æ–‡æ¡£ä¸­æ˜ç¡®æŒ‡å‡ºé’¢ç­‹ä¿æŠ¤å±‚åšåº¦ä¸º25mmï¼Œä½†æ˜¯è§„èŒƒä¸­å¹¶æ²¡æœ‰å…·ä½“è§„å®šåšåº¦ã€‚å› æ­¤ï¼Œè¿™ä¸ªç»†èŠ‚å¯èƒ½...\n",
      "âœï¸ ä¿®è®¢å»ºè®®: æ ¹æ®å®¡æŸ¥çš„ç»“æœï¼Œå¯¹æ–‡ä»¶è¿›è¡Œä¿®æ”¹å¦‚ä¸‹ï¼š\n",
      "\n",
      "1. æ··å‡åœŸå¼ºåº¦ç­‰çº§ä¸ä½äºC25ï¼šä¿ç•™æ­¤è¦æ±‚ï¼Œå› ä¸ºå®ƒåœ¨ä¸¤ä¸ªèµ„æ–™ä¸­éƒ½æœ‰æåˆ°ï¼Œå¹¶ä¸”ä¸ºç›¸åŒçš„æ ‡å‡†ã€‚\n",
      "2. é’¢ç­‹ä¿æŠ¤å±‚åšåº¦ä¸º25mmï¼šåˆ é™¤æ­¤æ¡æ¬¾ï¼Œå› ä¸ºè§„èŒƒä¸­å¹¶æ²¡æœ‰å…·ä½“è§„å®šé’¢ç­‹ä¿æŠ¤å±‚åšåº¦ï¼Œè¿™å¯èƒ½å­˜åœ¨å·®å¼‚ã€‚\n",
      "3. æ··å‡åœŸæµ‡ç­‘åº”è¿ç»­è¿›è¡Œï¼Œé—´æ­‡æ—¶é—´ä¸è¶…è¿‡1å°æ—¶ï¼šä¿®æ”¹ä¸ºâ€œæ··å‡åœŸ...\n",
      "ğŸ¯ ç½®ä¿¡åº¦: 0.90\n",
      "--------------------------------------------------\n",
      "\n",
      "â“ å®¡æŸ¥é—®é¢˜: æ˜¯å¦é’¢ç­‹æ··å‡åœŸæŸ±çš„å¼ºåº¦ç­‰çº§å·²ç»è¾¾åˆ°ä¸ä½äºC25çš„è¦æ±‚ï¼Œå¹¶ä¸”æ˜¯å¦æœ‰ç›¸å…³è¯æ®è¯æ˜äº†è¿™ä¸€ç‚¹ï¼Ÿ\n",
      "ğŸ“š æ£€ç´¢åˆ°çš„çŸ¥è¯†: 5ä¸ªç›¸å…³æ–‡æ¡£\n",
      "ğŸ” å¢å¼ºæŸ¥è¯¢: æ ¹æ®æä¾›çš„æ–‡æ¡£ï¼Œæˆ‘å¯ä»¥å¾—å‡ºä»¥ä¸‹ç­”æ¡ˆï¼š\n",
      "\n",
      "æ˜¯å¦é’¢ç­‹æ··å‡åœŸæŸ±çš„å¼ºåº¦ç­‰çº§å·²ç»è¾¾åˆ°ä¸ä½äºC25çš„è¦æ±‚ï¼Ÿ \n",
      "\n",
      "ç­”æ¡ˆï¼šæ˜¯çš„ã€‚ \n",
      "\n",
      "ä¾æ®Document 1ä¸­çš„â€œç„Šæ¥æ¥å¤´åº”æ»¡è¶³å¼ºåº¦è¦æ±‚â€ï¼Œå¯çŸ¥é’¢ç­‹ç„Šæ¥æŠ€æœ¯å·²ç»æ»¡è¶³äº†...\n",
      "âš ï¸ é”™è¯¯åˆ†æ: å®¡æŸ¥ç»“æœå¦‚ä¸‹ï¼š\n",
      "\n",
      "1. é’¢ç­‹æ··å‡åœŸæŸ±çš„å¼ºåº¦ç­‰çº§å·²ç»è¾¾åˆ°ä¸ä½äºC25çš„è¦æ±‚ã€‚ç›¸å…³è¯æ®è¯æ˜äº†è¿™ä¸€ç‚¹ï¼šæ ¹æ®é’¢ç­‹æ··å‡åœŸæŸ±çš„æ–½å·¥è§„èŒƒï¼Œæ··å‡åœŸå¼ºåº¦ç­‰çº§ä¸åº”ä½äºC25ã€‚\n",
      "\n",
      "2. æœ‰ç›¸åº”çš„è®°å½•å’Œè¯æ®è¡¨æ˜è¯¥å·¥ç¨‹çš„é’¢ç­‹ç„Šæ¥è´¨é‡è¿›è¡Œäº†æ£€éªŒï¼Œå¹¶ä¸”å…¶è¿æ¥æ¥å¤´æ»¡è¶³äº†å¼ºåº¦è¦æ±‚ã€‚è¿™ç¬¦åˆã€Šé’¢ç­‹ç„Šæ¥æŠ€æœ¯è¦æ±‚ã€‹çš„è§„å®šã€‚\n",
      "\n",
      "3. æ··å‡åœŸæµ‡...\n",
      "âœï¸ ä¿®è®¢å»ºè®®: å®¡æŸ¥ç»“æœï¼šåŸºäºç»™å‡ºçš„åˆ†æå’Œç›¸åº”çš„å‚è€ƒè§„èŒƒï¼Œé’¢ç­‹æ··å‡åœŸæŸ±çš„æ–½å·¥å®Œå…¨éµå®ˆäº†ç›¸å…³æ ‡å‡†ã€‚å› æ­¤ï¼Œä¸éœ€è¦è¿›è¡Œä»»ä½•ä¿®æ”¹ã€‚\n",
      "\n",
      "æœ€ç»ˆç»“è®ºï¼šæ ¹æ®å®¡æŸ¥ç»“æœï¼Œå¯ä»¥ç¡®è®¤é’¢ç­‹æ··å‡åœŸæŸ±çš„æ–½å·¥ç¬¦åˆã€Šé’¢ç­‹ç„Šæ¥æŠ€æœ¯è¦æ±‚ã€‹ã€ã€Šæ··å‡åœŸæµ‡ç­‘çš„æŠ€æœ¯è¦æ±‚ã€‹ã€ã€Šæ··å‡åœŸå…»æŠ¤è§„èŒƒã€‹å’Œã€Šæ¨¡æ¿å®‰è£…è§„èŒƒã€‹çš„è§„å®šã€‚...\n",
      "ğŸ¯ ç½®ä¿¡åº¦: 0.90\n",
      "--------------------------------------------------\n",
      "\n",
      "â“ å®¡æŸ¥é—®é¢˜: é’¢ç­‹ä¿æŠ¤å±‚åšåº¦æ˜¯å¦ç¬¦åˆ25mmçš„æ ‡å‡†ï¼Ÿæ˜¯å¦å­˜åœ¨è¿‡è–„æˆ–è¿‡åšçš„é—®é¢˜ï¼Ÿ\n",
      "ğŸ“š æ£€ç´¢åˆ°çš„çŸ¥è¯†: 5ä¸ªç›¸å…³æ–‡æ¡£\n",
      "ğŸ” å¢å¼ºæŸ¥è¯¢: ç­”æ¡ˆï¼š\n",
      "\n",
      "æ ¹æ®æä¾›çš„æ–‡æ¡£ä¿¡æ¯æ¥çœ‹ï¼Œé’¢ç­‹ä¿æŠ¤å±‚çš„åšåº¦æ˜¯å¦ç¬¦åˆ25mmçš„æ ‡å‡†æ˜¯æœ‰ä¸€å®šä¾æ®çš„ã€‚\n",
      "\n",
      "Document 1ä¸­æåˆ°â€œé’¢ç­‹ä¿æŠ¤å±‚åšåº¦åº”ç¬¦åˆè®¾è®¡è¦æ±‚â€ï¼Œè¿™æ„å‘³ç€é’¢ç­‹ä¿æŠ¤å±‚çš„åšåº¦åº”è¯¥æŒ‰ç…§å…·ä½“çš„è®¾è®¡è®¡åˆ’ç¡®...\n",
      "âš ï¸ é”™è¯¯åˆ†æ: æ ¹æ®æä¾›çš„å®¡æŸ¥æŸ¥è¯¢å’Œç›¸å…³å‚è€ƒè§„èŒƒï¼Œæˆ‘ä»¬å¯¹é’¢ç­‹æ··å‡åœŸæŸ±çš„æ–½å·¥è¿›è¡Œé”™è¯¯åˆ†æï¼š\n",
      "\n",
      "1.  é’¢ç­‹ä¿æŠ¤å±‚åšåº¦æ ‡å‡†ï¼š\n",
      "   - æ ¹æ®å‚è€ƒè§„èŒƒï¼Œé’¢ç­‹ä¿æŠ¤å±‚åšåº¦åº”ç¬¦åˆè®¾è®¡è¦æ±‚ï¼Œè€Œä¸æ˜¯å›ºå®šä¸º25mmã€‚\n",
      "   - å› æ­¤ï¼Œè¿™ä¸ªé—®é¢˜ä¸æ˜ç¡®ï¼Œéœ€è¦æ ¸å®è®¾è®¡æ–‡ä»¶ä»¥ç¡®è®¤å…·ä½“çš„åšåº¦è¦æ±‚ã€‚\n",
      "\n",
      "2. æ··å‡åœŸæµ‡ç­‘æŠ€æœ¯è¦æ±‚ï¼š\n",
      "   - é—®é¢˜...\n",
      "âœï¸ ä¿®è®¢å»ºè®®: æ ¹æ®ç»™å®šçš„åˆ†æå’Œå‚è€ƒè§„èŒƒï¼Œå¯¹æ‰€æä¾›çš„æ–‡ä»¶è¿›è¡Œå®¡æŸ¥å’Œä¿®æ”¹ã€‚è¦æ±‚ä¸¥æ ¼éµå®ˆæ‰€æä¾›çš„å‚è€ƒè§„èŒƒã€‚\n",
      "\n",
      "ä¿®è®¢å†…å®¹å¦‚ä¸‹ï¼š\n",
      "\n",
      "1. é’¢ç­‹ä¿æŠ¤å±‚åšåº¦æ ‡å‡†ï¼š\n",
      "   - ä¿®è®¢ä¸ºï¼šé’¢ç­‹ä¿æŠ¤å±‚åšåº¦åº”ç¬¦åˆè®¾è®¡è¦æ±‚ï¼Œè€Œä¸æ˜¯å›ºå®šä¸º25mmã€‚\n",
      "2. æ··å‡åœŸæµ‡ç­‘æŠ€æœ¯è¦æ±‚ï¼š\n",
      "   - ä¿®è®¢ä¸ºï¼šæµ‡ç­‘åº”è¿ç»­è¿›è¡Œï¼Œæµ‡ç­‘é—´æ­‡æ—¶é—´ä¸åº”è¶…è¿‡æ··å‡åœŸçš„åˆ...\n",
      "ğŸ¯ ç½®ä¿¡åº¦: 0.90\n",
      "--------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "ğŸ‰ GKGRç³»ç»Ÿå®ä¾‹åŒ–æˆåŠŸï¼\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š ç³»ç»Ÿé…ç½®æ€»ç»“ï¼š\n",
      "âœ… LLM: llama3.1:8b (Ollamaæœ¬åœ°éƒ¨ç½²)\n",
      "âœ… Embedding: BAAI/bge-m3 (Windowsä¼˜åŒ–ç‰ˆ)\n",
      "âœ… çŸ¥è¯†åº“: 5ä¸ªåŠ¨æ€åˆ†å—\n",
      "âœ… GKGRæ£€ç´¢: åŒé‡è¯„åˆ†æœºåˆ¶\n",
      "âœ… é”™è¯¯åˆ†æ: æ™ºèƒ½åå·®æ£€æµ‹\n",
      "âœ… ä¿®è®¢ç”Ÿæˆ: çŸ¥è¯†é©±åŠ¨ä¿®æ­£\n",
      "\n",
      "ğŸ—ï¸ å»ºç­‘æ–‡æ¡£æ™ºèƒ½å®¡æŸ¥ç³»ç»Ÿå·²å®Œå…¨å°±ç»ªï¼\n",
      "ğŸš€ å¯ä»¥å¼€å§‹å¤„ç†å®é™…çš„å»ºç­‘æ–‡æ¡£å®¡æŸ¥ä»»åŠ¡ï¼\n"
     ]
    }
   ],
   "source": [
    "# ğŸš€ ä¿®å¤åçš„å®Œæ•´GKGRç³»ç»Ÿæµ‹è¯•\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ¯ ä¿®å¤åçš„GKGRå»ºç­‘æ–‡æ¡£å®¡æŸ¥ç³»ç»Ÿæµ‹è¯•\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    # ========== 1. ä½¿ç”¨å·²åˆå§‹åŒ–çš„ç»„ä»¶ ==========\n",
    "    print(\"\\nğŸ“¦ Step 1: ä½¿ç”¨å·²åˆå§‹åŒ–çš„ç»„ä»¶...\")\n",
    "    print(f\"âœ… LLM: {llm.model_name} (Ollama)\")\n",
    "    print(f\"âœ… Embedding: {emb.model_name} (Windowsä¼˜åŒ–ç‰ˆ)\")\n",
    "    if hasattr(emb, 'is_dummy') and emb.is_dummy:\n",
    "        print(\"âš ï¸ æ³¨æ„ï¼šä½¿ç”¨è™šæ‹Ÿembeddingæ¨¡å¼ï¼ˆä»…ç”¨äºæ¶æ„æµ‹è¯•ï¼‰\")\n",
    "    \n",
    "    # ========== 2. åˆ›å»ºç®€å•çŸ¥è¯†åº“ï¼ˆæ›¿ä»£markdownæ–‡æ¡£ï¼‰ ==========\n",
    "    print(\"\\nğŸ“¦ Step 2: åˆ›å»ºç®€å•å»ºç­‘çŸ¥è¯†åº“...\")\n",
    "    \n",
    "    # ä½¿ç”¨ç®€å•çš„å»ºç­‘è§„èŒƒæ–‡æ¡£ä½œä¸ºçŸ¥è¯†åº“\n",
    "    simple_documents = [\n",
    "        \"\"\"é’¢ç­‹æ··å‡åœŸæŸ±çš„æ–½å·¥è§„èŒƒã€‚\n",
    "        æ··å‡åœŸå¼ºåº¦ç­‰çº§ä¸åº”ä½äºC25ã€‚\n",
    "        é’¢ç­‹ä¿æŠ¤å±‚åšåº¦åº”ç¬¦åˆè®¾è®¡è¦æ±‚ã€‚\n",
    "        æŸ±å­çš„å‚ç›´åº¦åå·®ä¸åº”è¶…è¿‡H/1000ä¸”ä¸å¤§äº20mmã€‚\"\"\",\n",
    "        \n",
    "        \"\"\"æ··å‡åœŸæµ‡ç­‘çš„æŠ€æœ¯è¦æ±‚ã€‚\n",
    "        æµ‡ç­‘åº”è¿ç»­è¿›è¡Œã€‚\n",
    "        æµ‡ç­‘é—´æ­‡æ—¶é—´ä¸åº”è¶…è¿‡æ··å‡åœŸçš„åˆå‡æ—¶é—´ã€‚\n",
    "        æ¯å±‚æµ‡ç­‘åšåº¦ä¸å®œè¶…è¿‡æŒ¯æ£å™¨ä½œç”¨é•¿åº¦çš„1.25å€ã€‚\"\"\",\n",
    "        \n",
    "        \"\"\"æ··å‡åœŸå…»æŠ¤è§„èŒƒã€‚\n",
    "        å…»æŠ¤æœŸé—´åº”ä¿æŒæ··å‡åœŸè¡¨é¢æ¹¿æ¶¦ã€‚\n",
    "        å…»æŠ¤æ—¶é—´ä¸å°‘äº7å¤©ã€‚\n",
    "        å¯¹äºé‡è¦ç»“æ„ï¼Œå…»æŠ¤æ—¶é—´åº”å»¶é•¿è‡³14å¤©ã€‚\"\"\",\n",
    "        \n",
    "        \"\"\"é’¢ç­‹ç„Šæ¥æŠ€æœ¯è¦æ±‚ã€‚\n",
    "        é’¢ç­‹ç„Šæ¥åº”ç¬¦åˆç›¸å…³è§„èŒƒè¦æ±‚ã€‚\n",
    "        ç„Šæ¥è´¨é‡åº”è¿›è¡Œæ£€éªŒã€‚\n",
    "        ç„Šæ¥æ¥å¤´åº”æ»¡è¶³å¼ºåº¦è¦æ±‚ã€‚\"\"\",\n",
    "        \n",
    "        \"\"\"æ¨¡æ¿å®‰è£…è§„èŒƒã€‚\n",
    "        æ¨¡æ¿å®‰è£…åº”ç‰¢å›ºã€‚\n",
    "        å‡ ä½•å°ºå¯¸åº”å‡†ç¡®ã€‚\n",
    "        è¡¨é¢åº”å¹³æ•´å…‰æ»‘ã€‚\"\"\"\n",
    "    ]\n",
    "    \n",
    "    # ä½¿ç”¨ä¿®å¤åçš„åŠ¨æ€è¯­ä¹‰åˆ†å—å™¨å¤„ç†æ–‡æ¡£\n",
    "    print(\"ğŸ”„ ä½¿ç”¨ä¿®å¤åçš„åŠ¨æ€è¯­ä¹‰åˆ†å—å™¨å¤„ç†æ–‡æ¡£...\")\n",
    "    knowledge_base = []\n",
    "    chunk_metadata = []\n",
    "    \n",
    "    for doc_idx, doc in enumerate(simple_documents):\n",
    "        # ä½¿ç”¨ä¿®å¤åçš„DynamicSemanticChunker\n",
    "        chunks = DynamicSemanticChunker(\n",
    "            embedding_model=emb,  # ä½¿ç”¨ä¼˜åŒ–è¿‡çš„embeddingæ¨¡å‹\n",
    "            max_chunk_length=100,\n",
    "            min_chunk_length=20\n",
    "        ).split_text(doc)\n",
    "        \n",
    "        for chunk_id, chunk_text in chunks.items():\n",
    "            knowledge_base.append(chunk_text)\n",
    "            chunk_metadata.append({\n",
    "                'doc_id': doc_idx,\n",
    "                'chunk_id': chunk_id,\n",
    "                'text': chunk_text\n",
    "            })\n",
    "    \n",
    "    print(f\"âœ… çŸ¥è¯†åº“æ„å»ºå®Œæˆï¼Œå…±{len(knowledge_base)}ä¸ªchunks\")\n",
    "    for i, chunk in enumerate(knowledge_base[:3]):\n",
    "        print(f\"  Chunk {i+1}: {chunk[:50]}...\")\n",
    "    \n",
    "    # ========== 3. åˆå§‹åŒ–ä½ çš„é«˜çº§RAGç»„ä»¶ ==========\n",
    "    print(\"\\nğŸ“¦ Step 3: åˆå§‹åŒ–é«˜çº§RAGç»„ä»¶...\")\n",
    "    \n",
    "    # ä½¿ç”¨ä½ çš„KeyInfoExtractor\n",
    "    key_extractor = KeyInfoExtractor(llm)\n",
    "    print(\"âœ… KeyInfoExtractoråˆå§‹åŒ–å®Œæˆ\")\n",
    "    \n",
    "    # ä½¿ç”¨ä½ çš„GKGRRetriever\n",
    "    gkgr_retriever = GKGRRetriever(\n",
    "        knowledge_base=knowledge_base,\n",
    "        embedding_model=emb,  # ä½¿ç”¨ä¼˜åŒ–è¿‡çš„embeddingæ¨¡å‹\n",
    "        key_info_extractor=key_extractor,\n",
    "        llm=llm,  # ä½¿ç”¨Ollamaçš„llama3.1:8b\n",
    "        config={\n",
    "            \"lambda_param\": 0.5,\n",
    "            \"top_k\": 5,\n",
    "            \"rerank_enabled\": True,\n",
    "            \"query_expansion\": True,\n",
    "            \"similarity_threshold\": 0.1\n",
    "        }\n",
    "    )\n",
    "    print(\"âœ… GKGRRetrieveråˆå§‹åŒ–å®Œæˆ\")\n",
    "    \n",
    "    # ä½¿ç”¨ä½ çš„ErrorAnalyzer\n",
    "    error_analyzer = ErrorAnalyzer(llm)\n",
    "    print(\"âœ… ErrorAnalyzeråˆå§‹åŒ–å®Œæˆ\")\n",
    "    \n",
    "    # ä½¿ç”¨ä½ çš„RevisionGenerator\n",
    "    revision_generator = RevisionGenerator(llm)\n",
    "    print(\"âœ… RevisionGeneratoråˆå§‹åŒ–å®Œæˆ\")\n",
    "    \n",
    "    # ========== 4. æµ‹è¯•ä½ çš„å®Œæ•´å®¡æŸ¥æµç¨‹ ==========\n",
    "    print(\"\\nğŸ“¦ Step 4: æµ‹è¯•å®Œæ•´å®¡æŸ¥æµç¨‹...\")\n",
    "    \n",
    "    # å¾…å®¡æŸ¥çš„æ–‡æ¡£å†…å®¹\n",
    "    sample_document = \"\"\"é’¢ç­‹æ··å‡åœŸæŸ±çš„æ–½å·¥åº”ç¬¦åˆä»¥ä¸‹è¦æ±‚ï¼š\n",
    "1. æ··å‡åœŸå¼ºåº¦ç­‰çº§ä¸ä½äºC25\n",
    "2. é’¢ç­‹ä¿æŠ¤å±‚åšåº¦ä¸º25mm\n",
    "3. æ··å‡åœŸæµ‡ç­‘åº”è¿ç»­è¿›è¡Œï¼Œé—´æ­‡æ—¶é—´ä¸è¶…è¿‡1å°æ—¶\n",
    "4. å…»æŠ¤æœŸé—´åº”ä¿æŒæ··å‡åœŸè¡¨é¢æ¹¿æ¶¦\"\"\"\n",
    "    \n",
    "    print(f\"ğŸ“„ å¾…å®¡æŸ¥æ–‡æ¡£ï¼š\\n{sample_document}\")\n",
    "    \n",
    "    # ä½¿ç”¨ä½ çš„complete_review_processå‡½æ•°æ‰§è¡Œå®Œæ•´å®¡æŸ¥\n",
    "    print(\"\\nğŸ”„ æ‰§è¡Œå®Œæ•´å®¡æŸ¥æµç¨‹...\")\n",
    "    result = complete_review_process(\n",
    "        sample_document, \n",
    "        gkgr_retriever, \n",
    "        error_analyzer, \n",
    "        revision_generator\n",
    "    )\n",
    "    \n",
    "    # ========== 5. å±•ç¤ºå®¡æŸ¥ç»“æœ ==========\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ğŸ“‹ å®¡æŸ¥ç»“æœ\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for query, analysis in result.items():\n",
    "        print(f\"\\nâ“ å®¡æŸ¥é—®é¢˜: {query}\")\n",
    "        print(f\"ğŸ“š æ£€ç´¢åˆ°çš„çŸ¥è¯†: {len(analysis['retrieved_knowledge'])}ä¸ªç›¸å…³æ–‡æ¡£\")\n",
    "        print(f\"ğŸ” å¢å¼ºæŸ¥è¯¢: {analysis['augmented_query'][:100]}...\")\n",
    "        print(f\"âš ï¸ é”™è¯¯åˆ†æ: {analysis['analysis']['analysis'][:150]}...\")\n",
    "        print(f\"âœï¸ ä¿®è®¢å»ºè®®: {analysis['revision']['revision_suggestions'][:150]}...\")\n",
    "        print(f\"ğŸ¯ ç½®ä¿¡åº¦: {analysis['revision']['confidence']:.2f}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    # ========== 6. ç³»ç»ŸçŠ¶æ€æ€»ç»“ ==========\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ğŸ‰ GKGRç³»ç»Ÿå®ä¾‹åŒ–æˆåŠŸï¼\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\nğŸ“Š ç³»ç»Ÿé…ç½®æ€»ç»“ï¼š\")\n",
    "    print(f\"âœ… LLM: {llm.model_name} (Ollamaæœ¬åœ°éƒ¨ç½²)\")\n",
    "    print(f\"âœ… Embedding: {emb.model_name} (Windowsä¼˜åŒ–ç‰ˆ)\")\n",
    "    print(f\"âœ… çŸ¥è¯†åº“: {len(knowledge_base)}ä¸ªåŠ¨æ€åˆ†å—\")\n",
    "    print(f\"âœ… GKGRæ£€ç´¢: åŒé‡è¯„åˆ†æœºåˆ¶\")\n",
    "    print(f\"âœ… é”™è¯¯åˆ†æ: æ™ºèƒ½åå·®æ£€æµ‹\")\n",
    "    print(f\"âœ… ä¿®è®¢ç”Ÿæˆ: çŸ¥è¯†é©±åŠ¨ä¿®æ­£\")\n",
    "    print(\"\\nğŸ—ï¸ å»ºç­‘æ–‡æ¡£æ™ºèƒ½å®¡æŸ¥ç³»ç»Ÿå·²å®Œå…¨å°±ç»ªï¼\")\n",
    "    print(\"ğŸš€ å¯ä»¥å¼€å§‹å¤„ç†å®é™…çš„å»ºç­‘æ–‡æ¡£å®¡æŸ¥ä»»åŠ¡ï¼\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ ç³»ç»Ÿå®ä¾‹åŒ–å¤±è´¥: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ba3b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = BGEEmbedding(model_name=\"BAAI/bge-m3\")\n",
    "key_extractor = KeyInfoExtractor(llm)\n",
    "\n",
    "# ä»markdownæ–‡æ¡£æ„å»ºçŸ¥è¯†åº“\n",
    "processor = DocumentProcessor()\n",
    "documents = processor.load_documents(\"./construction_standards\")\n",
    "\n",
    "# å¯¹æ–‡æ¡£è¿›è¡ŒåŠ¨æ€è¯­ä¹‰åˆ†å—\n",
    "chunker = DynamicSemanticChunker()\n",
    "knowledge_base = []\n",
    "for doc in documents:\n",
    "    chunks = chunker.split_text(doc)\n",
    "    knowledge_base.extend(chunks.values())\n",
    "\n",
    "# åˆå§‹åŒ–æ£€ç´¢å™¨\n",
    "gkgr_retriever = GKGRRetriever(\n",
    "    knowledge_base=knowledge_base,\n",
    "    embedding_model=embedding,\n",
    "    key_info_extractor=key_extractor,\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "# åˆå§‹åŒ–åˆ†æå™¨\n",
    "error_analyzer = ErrorAnalyzer(llm)\n",
    "revision_generator = RevisionGenerator(llm)\n",
    "\n",
    "# å¾…å®¡æŸ¥çš„æ–‡æ¡£å†…å®¹\n",
    "sample_document = \"\"\"\n",
    "é’¢ç­‹æ··å‡åœŸæŸ±çš„æ–½å·¥åº”ç¬¦åˆä»¥ä¸‹è¦æ±‚ï¼š\n",
    "1. æ··å‡åœŸå¼ºåº¦ç­‰çº§ä¸ä½äºC25\n",
    "2. é’¢ç­‹ä¿æŠ¤å±‚åšåº¦ä¸º25mm\n",
    "3. æ··å‡åœŸæµ‡ç­‘åº”è¿ç»­è¿›è¡Œï¼Œé—´æ­‡æ—¶é—´ä¸è¶…è¿‡1å°æ—¶\n",
    "4. å…»æŠ¤æœŸé—´åº”ä¿æŒæ··å‡åœŸè¡¨é¢æ¹¿æ¶¦\n",
    "\"\"\"\n",
    "\n",
    "# æ‰§è¡Œå®¡æŸ¥\n",
    "result = complete_review_process(\n",
    "    sample_document, \n",
    "    gkgr_retriever, \n",
    "    error_analyzer, \n",
    "    revision_generator\n",
    ")\n",
    "\n",
    "# æŸ¥çœ‹å®¡æŸ¥ç»“æœ\n",
    "for query, analysis in result.items():\n",
    "    print(f\"å®¡æŸ¥é—®é¢˜: {query}\")\n",
    "    print(f\"ä¿®è®¢å»ºè®®: {analysis['revision']['revision_suggestions']}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96070e33",
   "metadata": {},
   "source": [
    "## æ‰©å±•æ€§è¯´æ˜\n",
    "\n",
    "ç³»ç»Ÿå¯ä»¥é€šè¿‡æ›´æ¢çŸ¥è¯†åº“è½»æ¾é€‚åº”å…¶ä»–é¢†åŸŸã€‚å¯¹äºç‰¹å®šä¼ä¸šæˆ–é¡¹ç›®ï¼Œå¯ä»¥é€šè¿‡å¾®è°ƒå…³é”®ä¿¡æ¯æå–æ¨¡å‹æ¥æå‡å‡†ç¡®æ€§ã€‚åœ¨æ€§èƒ½ä¼˜åŒ–æ–¹é¢ï¼Œä½¿ç”¨åŠ¨æ€è¯­ä¹‰åˆ†å—å¯ä»¥æå‡æ£€ç´¢è´¨é‡ï¼Œé¢„è®¡ç®—å¹¶ç¼“å­˜çŸ¥è¯†åº“åµŒå…¥ä»¥æå‡æ£€ç´¢é€Ÿåº¦ï¼Œå¯¹äºå¤§é‡æ–‡æ¡£å¯ä½¿ç”¨æ‰¹é‡å¤„ç†æ¨¡å¼ï¼Œæ ¹æ®å…·ä½“åº”ç”¨åœºæ™¯è°ƒæ•´Î»å‚æ•°å’Œtop-kå€¼ã€‚\n",
    "\n",
    "## å†™åœ¨æœ€å\n",
    "\n",
    "æ­å–œä½ é˜…è¯»å®Œæ­¤æ–‡ï¼Œä½ å·²ç»å……åˆ†äº†è§£äº†å¦‚ä½•å®ç°ä¸€ä¸ªå»ºç­‘æ–‡æ¡£æ™ºèƒ½å®¡æŸ¥ç³»ç»Ÿä»¥åŠå…¶èƒŒåçš„æ€è€ƒã€‚è¿™ä¸ªç³»ç»Ÿå±•ç¤ºäº†å¦‚ä½•å°†åŠ¨æ€è¯­ä¹‰åˆ†å—ã€çŸ¥è¯†å¼•å¯¼æ£€ç´¢å’Œå¤§è¯­è¨€æ¨¡å‹æœ‰æœºç»“åˆï¼Œä¸ºå»ºç­‘è¡Œä¸šçš„æ–‡æ¡£å®¡æŸ¥æä¾›äº†ä¸€ä¸ªå®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚\n",
    "\n",
    "è™½ç„¶å½“å‰ç³»ç»Ÿå·²ç»å–å¾—äº†ä¸é”™çš„æ•ˆæœï¼Œä½†ä»æœ‰æ”¹è¿›ç©ºé—´ã€‚å…¨å±€å…³è”å¢å¼ºæ–¹é¢ï¼Œå½“å‰åŸºäºæ–‡æœ¬å—çš„æ£€ç´¢å¯ä»¥è¿›ä¸€æ­¥ç»“åˆçŸ¥è¯†å›¾è°±ç­‰æŠ€æœ¯ã€‚å¤šæ¨¡æ€æ”¯æŒæ–¹é¢ï¼Œæœªæ¥å¯ä»¥æ‰©å±•æ”¯æŒCADå›¾çº¸ã€æ–½å·¥å›¾ç­‰è§†è§‰ä¿¡æ¯ã€‚å®æ—¶æ›´æ–°æ–¹é¢ï¼Œæ”¯æŒçŸ¥è¯†åº“çš„å¢é‡æ›´æ–°å’ŒåŠ¨æ€ç»´æŠ¤ã€‚ä¸ªæ€§åŒ–å®šåˆ¶æ–¹é¢ï¼Œæ ¹æ®ä¸åŒä¼ä¸šå’Œé¡¹ç›®ç‰¹ç‚¹è¿›è¡Œç³»ç»Ÿå®šåˆ¶ã€‚\n",
    "\n",
    "è¯»è€…ä»¬å¯ä»¥è¿è¡Œé¡¹ç›®ä¸­çš„ç¤ºä¾‹ä»£ç ï¼Œä½“éªŒå®Œæ•´çš„å»ºç­‘æ–‡æ¡£æ™ºèƒ½å®¡æŸ¥æµç¨‹ã€‚æˆ‘ä»¬ç›¸ä¿¡è¿™ä¸ªç³»ç»Ÿä¸ä»…èƒ½å¤Ÿæå‡å®¡æŸ¥æ•ˆç‡ï¼Œæ›´èƒ½ä¸ºå»ºç­‘è¡Œä¸šçš„æ•°å­—åŒ–è½¬å‹è´¡çŒ®åŠ›é‡ã€‚\n",
    "\n",
    "## è‡´è°¢\n",
    "\n",
    "æœ¬é¡¹ç›®çš„å¼€å‘è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬æ·±å…¥ç ”ç©¶äº†å»ºç­‘å·¥ç¨‹é¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†å’Œæœ€æ–°çš„è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯ã€‚ç‰¹åˆ«æ„Ÿè°¢å»ºç­‘è¡Œä¸šä¸“å®¶æä¾›çš„å®è´µå»ºè®®ï¼Œä»¥åŠå¼€æºç¤¾åŒºåœ¨æŠ€æœ¯å®ç°æ–¹é¢çš„æ”¯æŒã€‚é¡¹ç›®ä»£ç å®ç°å‚è€ƒäº†LlamaIndexã€Transformersç­‰ä¼˜ç§€å¼€æºé¡¹ç›®çš„è®¾è®¡ç†å¿µã€‚\n",
    "\n",
    "éœ€è¦è¯´æ˜çš„æ˜¯ï¼Œæœ¬é¡¹ç›®ä¸“é—¨é’ˆå¯¹å»ºç­‘æ–½å·¥é¢†åŸŸçš„æ–‡æ¡£å®¡æŸ¥åœºæ™¯è¿›è¡Œäº†æ·±åº¦ä¼˜åŒ–ã€‚å¦‚æœæ‚¨éœ€è¦å¤„ç†å…¶ä»–é¢†åŸŸçš„æ–‡æ¡£ï¼Œå»ºè®®æ ¹æ®å…·ä½“éœ€æ±‚å¯¹ç³»ç»Ÿè¿›è¡Œç›¸åº”è°ƒæ•´ã€‚\n",
    "\n",
    "## æºç è·å–\n",
    "\n",
    "æœ¬é¡¹ç›®çš„æºç ä»¥åŠå®ä¾‹æ•°æ®å­˜æ”¾åœ¨ [GitHub ä»“åº“](https://github.com/Hongru0306/CDDRS)ã€‚\n",
    "\n",
    "## å¼•ç”¨\n",
    "\n",
    "å¦‚æœæ‚¨åœ¨ç ”ç©¶ä¸­ä½¿ç”¨äº†æœ¬é¡¹ç›®çš„æˆæœï¼Œè¯·æŒ‰å¦‚ä¸‹æ–¹å¼å¼•ç”¨ï¼š\n",
    "\n",
    "```bibtex\n",
    "@article{XIAO2025103618,\n",
    "  title = {Generative knowledge-guided review system for construction disclosure documents},\n",
    "  journal = {Advanced Engineering Informatics},\n",
    "  volume = {68},\n",
    "  pages = {103618},\n",
    "  year = {2025},\n",
    "  issn = {1474-0346},\n",
    "  doi = {https://doi.org/10.1016/j.aei.2025.103618},\n",
    "  url = {https://www.sciencedirect.com/science/article/pii/S1474034625005117},\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transfomers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
